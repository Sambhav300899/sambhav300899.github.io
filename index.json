[{"content":" Experience I am currently pursuing my Master\u0026rsquo;s in Computer Vision at Carnegie Mellon University. You can find my detailed CV here.\nCompany Link Role Dates Location xFarm Technologies ML Research Engineer Aug 2021-Aug 2025 Paris, France Spacesense (acquired by xFarm in 2024) ML Intern Feb 2021-Jul 2021 Paris, France Freelance ML Engineer Aug 2020-Oct 2020 Remote National Informatics Center ML Intern Dec 2019-Jan 2020 Delhi, India Dept. of Computer Applications, Manipal Institute of Technology Volunteer Researcher Aug 2019-Nov 2019 Manipal, India TATA Sons Computer Vision and Robotics Intern May 2019-Jul 2019 Bangalore, India RUGVED Systems AI Subteam Head Jan 2018-Dec 2018 Manipal, India Publications Title Authors Date [GEOINT 2023] Data Preparation Techniques for Robust ML/DL Training for Practical Remote Sensing Applications Sambhav Singh Rohatgi, Jyotsna Buddideti Apr 2023 [CCAI@NeurIPS 2022] Automating the creation of LULC datasets for semantic segmentation Sambhav Singh Rohatgi, Anthony Mucia Dec 2022 [Electronics11071151] Human Detection in Aerial Thermal Images Using Faster R-CNN and SSD Algorithms K. R. Akshatha, A. Kotegar Karunakar, Satish Shenoy, Abhilash Pai, Nikhil Nagaraj, Sambhav S. Rohatgi Apr 2022 ","date":"18 November 2024","externalUrl":null,"permalink":"/about/","section":"Sambhav Singh Rohatgi","summary":"\u003cstyle\u003e\n    .experience-table {\n        width: 100%;\n        border-collapse: collapse;\n    }\n    .experience-table th, .experience-table td {\n        padding: 10px;\n        text-align: left;\n        vertical-align: middle;\n    }\n    .experience-table .logo-cell {\n        width: 100px;\n        text-align: center;\n        vertical-align: middle;\n        display: table-cell;\n    }\n    .experience-table img {\n        max-width: 80px; /* Adjust size as needed */\n        height: auto;\n    }\n    .experience-table td[data-label=\"Dates\"] {\n        min-width: 10px; /* Added minimum width for dates column */\n    }\n    /* Add media query for responsiveness */\n    @media (max-width: 600px) {\n        .experience-table, .experience-table thead, .experience-table tbody, .experience-table th, .experience-table td, .experience-table tr {\n            display: block;\n            width: 100%;\n        }\n        .experience-table thead {\n            display: none; /* Hide table header on mobile */\n        }\n        .experience-table tr {\n            margin-bottom: 15px;\n        }\n        .experience-table td {\n            text-align: right;\n            padding-left: 50%;\n            position: relative;\n        }\n        .experience-table td::before {\n            content: attr(data-label);\n            position: absolute;\n            left: 0;\n            width: 50%;\n            padding-left: 10px;\n            font-weight: bold;\n            text-align: left;\n        }\n        .experience-table td[data-label=\"Company\"]::before {\n            content: none; \n        }\n        .experience-table .logo-cell {\n            text-align: center; /* Center align logos on mobile */\n            padding-right: 0; /* Remove padding for logo cells */\n            padding-left: 0;\n        }\n    }\n\u003c/style\u003e\n\n\u003ch2 class=\"relative group\"\u003eExperience \n    \u003cdiv id=\"experience\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eI am currently pursuing my Master\u0026rsquo;s in Computer Vision at Carnegie Mellon University. You can find my detailed CV \u003ca href=\"https://drive.google.com/file/d/1MJmAwludn_qDEmETcdabf_bKFYIlbomV/view\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"","type":"page"},{"content":"","date":"18 November 2024","externalUrl":null,"permalink":"/","section":"Sambhav Singh Rohatgi","summary":"","title":"Sambhav Singh Rohatgi","type":"page"},{"content":" Motivation # I\u0026rsquo;m working as an industry expert on a research project at UC Irvine, led by Dr. Nadia Ahmed. Our team is studying how satellite imagery can be used to track animal migration. To help my teammates understand the process of geospatial dataset creation, I created a flowchart. This flowchart gives a clear picture of the steps involved in preparing geospatial datasets for Machine Learning.\nExplanation and recording # I ended up giving an impromptu lecture on it for the team. This included an explanation of my NeurIPS 2023 CCAI tutorial: Automating the creation of LULC datasets for semantic segmentation as well. I have divided the recording into sections for easier viewing. I have put the tutorial recording at the end of the page.\nFlowchart # Keep in mind that the data used in this flowchart is hypothetical, and some abstractions/assumptions were made to make it easier to understand.\nI would recommend viewing it in a separate browser window.\nP.S - Miro is awesome!\nflowchart # flowchart as slides # Automating the creation of LULC datasets for semantic segmentation # ","date":"25 September 2024","externalUrl":null,"permalink":"/blog/geospatial-dataset-creation-sample-pipeline/","section":"My Personal Blog","summary":"\u003ciframe width=\"650\" height=\"315\" src=\"https://www.youtube.com/embed/irj_v3uHLy4?si=abe65Ny-Ds_coZh2\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\u003ch2 class=\"relative group\"\u003eMotivation \n    \u003cdiv id=\"motivation\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\" href=\"#motivation\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m working as an industry expert on a research project at \u003ca\n  href=\"https://uci.edu/\"\n    target=\"_blank\"\n  \u003eUC Irvine\u003c/a\u003e, led by \u003ca\n  href=\"https://www.linkedin.com/in/nadia-a/\"\n    target=\"_blank\"\n  \u003eDr. Nadia Ahmed\u003c/a\u003e. Our team is studying how satellite imagery can be used to track animal migration. To help my teammates understand the process of geospatial dataset creation, I created a flowchart. This flowchart gives a clear picture of the steps involved in preparing geospatial datasets for Machine Learning.\u003c/p\u003e","title":"A pipeline for creating Geospatial AI datasets","type":"blog"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/geospatial/","section":"Tags","summary":"","title":"Geospatial","type":"tags"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning","type":"tags"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/blog/","section":"My Personal Blog","summary":"","title":"My Personal Blog","type":"blog"},{"content":"","date":"25 September 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"11 August 2024","externalUrl":null,"permalink":"/tags/gcp/","section":"Tags","summary":"","title":"Gcp","type":"tags"},{"content":"","date":"11 August 2024","externalUrl":null,"permalink":"/series/mlflow-on-gcp/","section":"Series","summary":"","title":"MLflow on GCP","type":"series"},{"content":"","date":"11 August 2024","externalUrl":null,"permalink":"/tags/mlops/","section":"Tags","summary":"","title":"Mlops","type":"tags"},{"content":" Sambhav300899/mlflow-gcp-server Python 1 0 Motive # In part 1 of the series, we looked at how we can host our own tracking server on GCP using a VM, we did see many issues with the earlier approach though, with regards to authentication, versioning, manual resource management etc. In this article, we will look at how we can use cloud run to host our server, some of the advantages that cloud run provides is that we don\u0026rsquo;t need to worry about scalability and idle resources, along with providing a cleaner way for containerisation.\nCreate backend DB # We start with the same step of creating a cloud SQL DB to store our data. To do this -\nGo to the cloud SQL page Click on create instance Click on create a PostgreSQL instance Fill in the instance ID and password. e.g. - I used \u0026ldquo;mlflow-backend-db\u0026rdquo; as the instance name and \u0026ldquo;test\u0026rdquo; as the password for now. When creating an DB instance, GCP creates the admin user as \u0026ldquo;postgres\u0026rdquo; by default, the entered password is linked to this admin username. For now select DB params and configuration to minimise cost, for example, I selected a custom config of 1 vCPU and 3.75 GB RAM. Also set storage capacity to 10 GB and make it single region. Under the connections settings, make it so that private IP is ticked. Under associated networking, select the default network. Under `authorized networks add 0.0.0.0/0. This gives access to the DB from any IP address on the internet. You can leave the rest as is. Click on create instance. The instance will take some time to get created. A default database named \u0026ldquo;postgres\u0026rdquo; already exists, create a new db named \u0026ldquo;mlflow\u0026rdquo; (change the name according to your needs). To do this, go to the Databases section after selecting your instance and click on create database. Create GCS bucket # Create a GCS bucket to store our model training artifacts. You can use the following command -\ngsutil mb gs://{ARTIFACT_BUCKET_NAME} I used the bucket name \u0026ldquo;mlflow-sample-bucket\u0026rdquo;. Keep in mind GCS bucket names are globally unique, so you will need to enter a different name.\nArtifact registry # We will need a place to store our docker images, for that we will need to create a new repo.\ngcloud artifacts repositories create {REPO_NAME} \\ --location=us-central1 \\ --repository-format=docker I used \u0026ldquo;mlflow-runner\u0026rdquo; as the name for now.\nService account # Our application will need access to cloud SQL, cloud storage and cloud run. For this we will be using a service account.\ngcloud iam service-accounts create {SERVICE_ACC_NAME} I used \u0026ldquo;mlflow-sa\u0026rdquo; for now.\nWe also have to give our service permissions and roles to be able to access different services. Run the following to give these permissions.\ngcloud projects add-iam-policy-binding PROJECT_ID \u0026ndash;member=\u0026lsquo;serviceAccount:SA-NAME@PROJECT_ID.iam.gserviceaccount.com\u0026rsquo; \u0026ndash;role=\u0026lsquo;ROLE_NAME\u0026rsquo;\n## RUN THESE FIRST export PROJECT_ID={GCP_PROJECT_ID} export SA_NAME={SERVICE_ACC_NAME} ## You can run all of these together gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member=\u0026#34;serviceAccount:$SA_NAME@$PROJECT_ID.iam.gserviceaccount.com\u0026#34; \\ --role=\u0026#34;roles/cloudsql.editor\u0026#34; gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member=\u0026#34;serviceAccount:$SA_NAME@$PROJECT_ID.iam.gserviceaccount.com\u0026#34; \\ --role=\u0026#39;roles/storage.objectAdmin\u0026#39; gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member=\u0026#34;serviceAccount:$SA_NAME@$PROJECT_ID.iam.gserviceaccount.com\u0026#34; \\ --role=\u0026#39;roles/secretmanager.secretAccessor\u0026#39; gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member=\u0026#34;serviceAccount:$SA_NAME@$PROJECT_ID.iam.gserviceaccount.com\u0026#34; \\ --role=\u0026#39;roles/artifactregistry.admin\u0026#39; gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member=\u0026#34;serviceAccount:$SA_NAME@$PROJECT_ID.iam.gserviceaccount.com\u0026#34; \\ --role=\u0026#39;roles/cloudfunctions.admin\u0026#39; gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member=\u0026#34;serviceAccount:$SA_NAME@$PROJECT_ID.iam.gserviceaccount.com\u0026#34; \\ --role=\u0026#39;roles/clouddeploy.serviceAgent\u0026#39; Get SA key # Create the creds.json file with the following command. Not that this creates the file in the root folder that the command is being run in. We will use these credentials as a secret in the secret manager.\ngcloud iam service-accounts keys create creds.json --iam-account=$SA_NAME@$PROJECT_ID.iam.gserviceaccount.com Add SA key to Secret manager # Use the following to create a secret variable called mlflow_runner_key using the creds.json we generated above.\ngcloud secrets create mlflow_runner_key --data-file=creds.json We will also create secret variables for the the DB url, bucket name and mlflow credentials.\nDB_USER_NAME = postgres (remember this is the default admin name) DB_USER_PASSWORD = test (you might have entered something different) DB_PRIVATE_IP = can be found in the overview page of the DB DB_NAME = mlflow (you might have entered something different) MLFLOW_ID = any string works (username for authentication) MLFLOW_PASS = any string works (password for authentication) gcloud secrets create DB_url printf \u0026#34;postgresql://{DB_USER_NAME}:{DB_USER_PASSWORD}@{DB_PUBLIC_IP}/{DB_NAME}\u0026#34; |gcloud secrets versions add DB_url --data-file=- For the bucket\nARTIFACT_BUCKET_NAME=mlflow-sample-bucket gcloud secrets create bucket_url printf \u0026#34;gs://{ARTIFACT_BUCKET_NAME}\u0026#34; | gcloud secrets versions add bucket_url --data-file=- For Mlflow ID and Password\ngcloud secrets create mlflow_id gcloud secrets create mlflow_pass printf \u0026#34;{MLFLOW_ID}\u0026#34; | gcloud secrets versions add mlflow_id --data-file=- printf \u0026#34;{MLFLOW_PASS}\u0026#34; | gcloud secrets versions add mlflow_pass --data-file=- Afterwards, we will be using these names in our github actions workflow.\nGithub Repository # I have created a repository for the project, linked below - Sambhav300899/mlflow-gcp-server Python 1 0 Add secrets to github repo # We need to add a \u0026ldquo;secret\u0026rdquo; to our github repo, so that github actions can use service account to run the workflow. To add the creds.json we created for our service account in the Get SA key section to our repo secrets -\ngo to repo settings\u0026gt;Security\u0026gt;Secrets and Variables\u0026gt;Actions click on New repository secret Name it GOOGLE_APPLICATION_CREDENTIALS and copy the contents of creds.json to it. Repository structure # The structure of our repo is -\n. ├── .github/ │ └── workflows/ | └── deploy_cloud_run.yml ├── .gitignore ├── Dockerfile ├── README.md ├── requirements.txt ├── server.sh ├── test.py └── wsgi_server.py We will go through each of these files except the gitignore, requirements and README.\nwsgi_server.py # This file wraps the mlflow server as an app inside the BasicAuth authentication. This is used to do very simple username and password based authentication.\nFor the username and password, we set them up as environment variables in the Dockerfile.\nimport os from wsgi_basic_auth import BasicAuth from mlflow.server import app as mlflow_app app = BasicAuth(mlflow_app) server.sh # Here we setup some of the internally used env variables to pass the location of our database and GCS bucket to mlflow. We also setup the authentication credentials. The variables being used here are passed during runtime through cloud run.\nAfter setting up the variables, we launch a Gunicorn web server which serves our python app, which is the mlflow server. 0.0.0.0:5000 means it will listen to connections from all IP addresses on port 5000.\n#!/bin/bash export _MLFLOW_SERVER_FILE_STORE=$POSTGRESQL_URL export _MLFLOW_SERVER_SERVE_ARTIFACTS=\u0026#34;true\u0026#34; export _MLFLOW_SERVER_ARTIFACT_DESTINATION=$STORAGE_URL export _MLFLOW_SERVER_ARTIFACT_ROOT=$STORAGE_URL export WSGI_AUTH_CREDENTIALS=\u0026#34;${MLFLOW_ID}:${MLFLOW_PASS}\u0026#34; exec gunicorn -b \u0026#34;0.0.0.0:5000\u0026#34; wsgi_server:app Dockerfile # The dockerfile\nloads environment variables copies the required files to our workdir installs the requirements using pip Exposes port 5000 since that is the port the mlflow server is listening for connections on. Runs the server.sh file. FROM python:3.11-slim WORKDIR / ENV GOOGLE_APPLICATION_CREDENTIALS=\u0026#39;./secrets/credentials\u0026#39; COPY requirements.txt requirements.txt COPY wsgi_server.py wsgi_server.py COPY server.sh server.sh RUN pip install --upgrade pip \u0026amp;\u0026amp; pip install -r requirements.txt EXPOSE 5000 RUN chmod +x server.sh ENTRYPOINT [\u0026#34;./server.sh\u0026#34;] deploy_cloud_run.yml # The yml file is used to define the github actions workflow. The workflow is reponsible for deploying our container to cloud run. I have added comments to the different parts to annotate them below.\n# Build and push container to artifact register, deploy on cloud run # the name of the workflow name: Deploy MLFlow server # Trigger the workflow when we push to \u0026#34;main\u0026#34; on: push: branches: [main] # jobs: login-build-push: name: Build, Push and Run runs-on: ubuntu-latest ############## INSERT YOUR OWN ENV VARIABLES HERE ######################## env: REGION: us-central1 PROJECT_ID: hardy-symbol-432213-a7 REPOSITORY: mlflow-runner SERVICE_ACCOUNT: mlflow-sa@hardy-symbol-432213-a7.iam.gserviceaccount.com SERVICE_NAME: mlflow-runner PORT: 5000 SA_KEY_NAME: mlflow_runner_key ############################################################################ steps: # checkout code from the github repo - name: Checkout uses: actions/checkout@v3 # Authenticate to GCP using our service account credentials and project ID - id: \u0026#39;auth\u0026#39; name: Authenticate to GCP uses: google-github-actions/auth@v1 with: project_id: \u0026#39;${{ env.PROJECT_ID }}\u0026#39; # we stored the credentials json as a github repository secret credentials_json: \u0026#39;${{ secrets.GOOGLE_APPLICATION_CREDENTIALS }}\u0026#39; # Configures Docker to use GCPs artifact repository for our region - name: Configure Docker to use gcloud as a credential helper run: |- gcloud auth configure-docker \u0026#34;${{ env.REGION }}-docker.pkg.dev\u0026#34; # Build the Docker image - name: \u0026#39;Docker config\u0026#39; run: |- docker build -t \u0026#34;${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/mlflow:${{ github.sha }}\u0026#34; . # Push the image to the artifact registry - name: \u0026#39;Push container\u0026#39; run: |- docker push \u0026#34;${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/mlflow:${{ github.sha }}\u0026#34; # Deploy the docker image to cloud run # We also provide the path to the secrets being used, and what env variables to assign them to - name: \u0026#39;Deploy to cloud run\u0026#39; run: |- gcloud run deploy \u0026#34;${{ env.SERVICE_NAME }}\u0026#34; \\ --image \u0026#34;${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/mlflow:${{ github.sha }}\u0026#34; \\ --region \u0026#34;${{ env.REGION }}\u0026#34; \\ --service-account \u0026#34;${{ env.SERVICE_ACCOUNT }}\u0026#34; \\ --update-secrets=/secrets/credentials=\u0026#34;${{ env.SA_KEY_NAME }}\u0026#34;:latest \\ --update-secrets=POSTGRESQL_URL=DB_url:latest \\ --update-secrets=STORAGE_URL=bucket_url:latest \\ --update-secrets=MLFLOW_ID=mlflow_id:latest \\ --update-secrets=MLFLOW_PASS=mlflow_pass:latest \\ --memory 2Gi \\ --allow-unauthenticated \\ --port \u0026#34;${{ env.PORT }}\u0026#34; test.py # A sample script to test out our MLflow server once deployed. Insert your credentials and MLflow tracking URI before running it.\nThe tracking uri can be found on your cloud run service page.\nimport os import mlflow import mlflow.sklearn from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression import numpy as np from sklearn.metrics import mean_squared_error, r2_score # Generate random data np.random.seed(42) X = np.random.rand(100, 1) y = 3 * X.squeeze() + 2 + np.random.randn(100) * 0.5 ############## Set MLflow tracking URI ############## os.environ[\u0026#34;MLFLOW_TRACKING_USERNAME\u0026#34;] = \u0026#34;\u0026#34; os.environ[\u0026#34;MLFLOW_TRACKING_PASSWORD\u0026#34;] = \u0026#34;\u0026#34; mlflow.set_tracking_uri(\u0026#34;\u0026#34;) ##################################################### # Split the data X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42 ) # Start an MLflow run with mlflow.start_run(): model = LinearRegression() model.fit(X_train, y_train) predictions = model.predict(X_test) mlflow.log_param(\u0026#34;fit_intercept\u0026#34;, model.fit_intercept) mse = mean_squared_error(y_test, predictions) r2 = r2_score(y_test, predictions) mlflow.log_metric(\u0026#34;mse\u0026#34;, mse) mlflow.log_metric(\u0026#34;r2\u0026#34;, r2) mlflow.sklearn.log_model(model, \u0026#34;linear_regression_model\u0026#34;) print(f\u0026#34;Model saved in run {mlflow.active_run().info.run_id}\u0026#34;) print(f\u0026#34;MSE: {mse}\u0026#34;) print(f\u0026#34;R2: {r2}\u0026#34;) Testing it out # You can get the link of your MLflow server from the cloud run service page! Once you have this, enter it in your browser.\nYou should be prompted for your username and ID as follows - Enter your username and password to access the dashboard. Enter your parameters into the test.py script and run it, you should be able to see your outputs in the dashboard if everything is working properly! And we\u0026rsquo;re all done!\nReferences # https://medium.com/@andrevargas22/how-to-launch-an-mlflow-server-with-continuous-deployment-on-gcp-in-minutes-7d3a29feff88 https://dlabs.ai/blog/a-step-by-step-guide-to-setting-up-mlflow-on-the-google-cloud-platform/ ","date":"11 August 2024","externalUrl":null,"permalink":"/blog/mlflow-on-gcp-2/","section":"My Personal Blog","summary":"\u003cdiv class=\"github-card-wrapper\"\u003e\n    \u003ca id=\"github-6a37d74da3198b2f805514c8929e5016\" target=\"_blank\" href=\"https://github.com/Sambhav300899/mlflow-gcp-server\" class=\"cursor-pointer\"\u003e\n      \u003cdiv\n        class=\"w-full md:w-auto p-0 m-0 border border-neutral-200 dark:border-neutral-700 border rounded-md shadow-2xl\"\u003e\u003cdiv class=\"w-full nozoom\"\u003e\n            \u003cimg\n              src=\"https://opengraph.githubassets.com/0/Sambhav300899/mlflow-gcp-server\"\n              alt=\"GitHub Repository Thumbnail\"\n              class=\"nozoom mt-0 mb-0 w-full h-full object-cover\"\u003e\n          \u003c/div\u003e\u003cdiv class=\"w-full md:w-auto pt-3 p-5\"\u003e\n          \u003cdiv class=\"flex items-center\"\u003e\n            \u003cspan class=\"text-2xl text-neutral-800 dark:text-neutral me-2\"\u003e\n              \u003cspan class=\"relative block icon\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 496 512\"\u003e\u003cpath fill=\"currentColor\" d=\"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z\"/\u003e\u003c/svg\u003e\n\u003c/span\u003e\n            \u003c/span\u003e\n            \u003cdiv\n              id=\"github-6a37d74da3198b2f805514c8929e5016-full_name\"\n              class=\"m-0 font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral\"\u003e\n              Sambhav300899/mlflow-gcp-server\n            \u003c/div\u003e\n          \u003c/div\u003e\n\n          \u003cp id=\"github-6a37d74da3198b2f805514c8929e5016-description\" class=\"m-0 mt-2 text-md text-neutral-800 dark:text-neutral\"\u003e\n            \n          \u003c/p\u003e","title":"Part 2: Deploy your custom MLflow server on GCP with Cloud run and github actions","type":"blog"},{"content":"","date":"11 August 2024","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":" What is this? # This is my first journey of building barbershop AI, an app to try out different hairstyles virtually.\nI\u0026rsquo;ve wanted to work on this for a while now and buildspace\u0026rsquo;s nights \u0026amp; weekends S5 seems like the perfect platform to do so.\n","date":"1 July 2024","externalUrl":null,"permalink":"/barbershop_ai/","section":"Building Barbershop AI","summary":"\u003ch2 class=\"relative group\"\u003eWhat is this? \n    \u003cdiv id=\"what-is-this\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\" href=\"#what-is-this\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThis is my first journey of building barbershop AI, an app to try out different hairstyles virtually.\u003c/p\u003e","title":"Building Barbershop AI","type":"barbershop_ai"},{"content":"This section contains notes from papers I have read, these can be very unrefined and unedited.\n","date":"28 June 2024","externalUrl":null,"permalink":"/papernotes/","section":"Paper Reading Notes","summary":"\u003cp\u003eThis section contains notes from papers I have read, these can be very unrefined and unedited.\u003c/p\u003e","title":"Paper Reading Notes","type":"papernotes"},{"content":" Motive # MLflow is a popular library for experiment tracking for Machine Learning. In addition to experiment tracking, it also offers a model registery and tracking platform. MLFlow can be used with a local database and artifact store, this is usually good enough for running experiments personally. When we need multiple people to work on the same experiment or run training as a job, we need to host a remote MLflow server.\nIn this post we will look at how we can host a MLflow server on GCP. In part 1 of this 2 part series, we will first look at how to host the remote server with a VM.\nThe aim of this post is to create a reusable script, so that we don\u0026rsquo;t have to go through individual steps manually. We will only do the full script in the final post though, for now, we are going to use the UI where possible.\nCreate backend DB # We need to a cloud SQL database to store our data. This can be done by the following steps-\nGo to the cloud SQL page Click on create instance Click on create a PostgreSQL instance Fill in the instance ID and password. e.g. - I used \u0026ldquo;mlflow-backend-db\u0026rdquo; as the instance name and \u0026ldquo;test\u0026rdquo; as the password for now. When creating an DB instance, GCP creates the admin user as \u0026ldquo;postgres\u0026rdquo; by default, the entered password is linked to this admin username. For now select DB params and configuration to minimise cost, for example, I selected a custom config of 1 vCPU and 3.75 GB RAM. Also set storage capacity to 10 GB and make it single region. Under the connections settings, make it so that private IP is ticked. Under associated networking, select the default network. You can leave the rest as is. Click on create instance. The instance will take some time to get created. A default database named \u0026ldquo;postgres\u0026rdquo; already exists, create a new db named \u0026ldquo;mlflow\u0026rdquo; (change the name according to your needs). To do this, go to the Databases section after selecting your instance and click on create database. Create GCS bucket # Create a GCS bucket to store our model training artifacts. You can use the following command -\ngsutil mb gs://{ARTIFACT_BUCKET_NAME} I used the bucket name \u0026ldquo;mlflow-sample-bucket\u0026rdquo;. Keep in mind GCS bucket names are globally unique, so you will need to enter a different name.\nCreate VM instance # To create a VM instance for our use -\nGo to the VM instance page Click on create instance Select an E2 instance. Under boot disk, click on change. Here select the \u0026ldquo;Deep Learning for Linux\u0026rdquo; OS. Under Identity and API access select Allow full access to all Cloud APIs. Under advanced_options/networking, add a \u0026ldquo;mlflow\u0026rdquo; network tag. This will be useful to create a firewall and apply it to only VM\u0026rsquo;s with this tag. Click on create. Add firewall rule # We need to add a firewall rule to allow incoming connections when we run the server.\ngcloud compute --project={your-project-id} \\ firewall-rules create mlflow \\ --direction=INGRESS \\ --priority=1000 \\ --network=default \\ --action=ALLOW \\ --rules=tcp:5000 \\ --source-ranges=0.0.0.0/0 \\ --target-tags=mlflow We set the port to allow these connections to 5000, as that is the port mlflow uses by default. For the target-tags argument, we pass the mlflow tag, since that is the tag we assigned to our VM. You can also go and do this through the UI. The source range 0.0.0.0/0 allows anyone on the internet to access this.\nRun the MLflow server on the VM # To run the MLflow server on the VM, connect to your VM using ssh and run the following commands-\npip install mlflow psycopg2-binary The next command is of the form\nmlflow server \\ -h 0.0.0.0 \\ -p 5000 \\ --backend-store-uri postgresql://{DB_USER_NAME}:{DB_USER_PASSOWRD}@{DB_PRIVATE_IP}:5432/mlflow \\ --default-artifact-root gs://{ARTIFACT_BUCKET_NAME} From our previous commands, we know the following -\nDB_USER_NAME=postgres (remember this is the default admin name) DB_USER_PASSWORD=test DB_PRIVATE_IP=can be found in the overview page of the DB ARTIFACT_BUCKET_NAME=mlflow-sample-bucket mlflow server \\ -h 0.0.0.0 \\ -p 5000 \\ --backend-store-uri postgresql://postgres:test@{private-ip}:5432/mlflow \\ --default-artifact-root gs://mlflow-sample-bucket We specify -h as 0.0.0.0 to listen on all available network interfaces, we specify -p as 5000 for the port to listen on. The 5432 port is the default port used by the DB here.\nAccess the MLflow dashboard # You should now be able to use the mlflow server by accessing port 5000 of the external IP of your VM instance. This can be found in the VM instances page.\nType the following in your browser address bar-\n{externel-ip}:5000 et voila! Running a sample experiment # So how can we actually include this in our code when running an experiment? It is very simple, just add a line to include the address of your tracking and you\u0026rsquo;re good. Look at the following very simple example for how to add it, other than that your code remains unchanged.\nSidenote, ensure that the instance you run this on (local or VM) has a GCP account with access to write to the bucket we created above. If it does and you are having authentication issues, run the following (usually solves default authentication issue for me, but you might need to debug further) -\ngcloud auth login gcloud auth application-default login import mlflow import mlflow.sklearn from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression import numpy as np from sklearn.metrics import mean_squared_error, r2_score # Generate random data np.random.seed(42) X = np.random.rand(100, 1) y = 3 * X.squeeze() + 2 + np.random.randn(100) * 0.5 ############## Set MLflow tracking URI ############## mlflow.set_tracking_uri(\u0026#34;http://\u0026lt;vm-external-ip\u0026gt;:5000\u0026#34;) ##################################################### # Split the data X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42 ) # Start an MLflow run with mlflow.start_run(): model = LinearRegression() model.fit(X_train, y_train) predictions = model.predict(X_test) mlflow.log_param(\u0026#34;fit_intercept\u0026#34;, model.fit_intercept) mse = mean_squared_error(y_test, predictions) r2 = r2_score(y_test, predictions) mlflow.log_metric(\u0026#34;mse\u0026#34;, mse) mlflow.log_metric(\u0026#34;r2\u0026#34;, r2) mlflow.sklearn.log_model(model, \u0026#34;linear_regression_model\u0026#34;) print(f\u0026#34;Model saved in run {mlflow.active_run().info.run_id}\u0026#34;) print(f\u0026#34;MSE: {mse}\u0026#34;) print(f\u0026#34;R2: {r2}\u0026#34;) After this your run should be visible on your mlflow dashboard (\u0026lt;external-ip\u0026gt;:5000) and would look something like the following - Issues with current approach # So we got this working, but there are many problems with this implementation, such as -\nWe have no authentication, anyone can access this dashboard, not such a great approach. We have no set version of mlflow being used, it might make more sense to use a more containerised approach. Scaling would be an issue, we will manually have to manage the VM. We also pay for idle resources during low traffic. In part 2, we will try to find an approach that solves this issue.\n","date":"16 June 2024","externalUrl":null,"permalink":"/blog/mlflow-on-gcp-1/","section":"My Personal Blog","summary":"\u003c!-- Contents-\n- [Motive](#motive)\n- [Create backed DB](#create backend DB)\n- [Create GCS bucket](#create GCS bucket)\n- [Create VM instance](#create VM instance)\n- [Add firewall rule](#add firewall rule)\n- [Run the MLflow server on the VM](#Run the MLflow server on the VM)\n- [Access the MLflow dashboard](#Access the MLflow dashboard)\n- [Running a sample experiment](#Running a sample experiment)\n- [Issues with current approach ](#Issues with current approach )\n--\u003e\n\n\u003ch2 class=\"relative group\"\u003eMotive \n    \u003cdiv id=\"motive\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\" href=\"#motive\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eMLflow is a popular library for experiment tracking for Machine Learning. In addition to experiment tracking, it also offers a model registery and tracking platform. MLFlow can be used with a local database and artifact store, this is usually good enough for running experiments personally. When we need multiple people to work on the same experiment or run training as a job, we need to host a remote MLflow server.\u003c/p\u003e","title":"Part 1: Hosting your custom MLflow server on GCP VM","type":"blog"},{"content":"","date":"16 July 2023","externalUrl":null,"permalink":"/tags/object-detection/","section":"Tags","summary":"","title":"Object Detection","type":"tags"},{"content":"","date":"16 July 2023","externalUrl":null,"permalink":"/tags/paper-notes/","section":"Tags","summary":"","title":"Paper Notes","type":"tags"},{"content":"These are notes for my reading of the YOLO v5 paper. Check out the orignal paper at the link below\nYOLO v4 High Level # They create a CNN that operates in real time on a conventional GPU and can be trained on a single GPU as well.\nTheir main contributions are as follows -\nDevelop and efficient and powerful object detection model. It means conventional GPUs such as a 1080 Ti or 2080 Ti can be used to train the model. They look at the SOTA Bag-of-Freebies and Bag-of-Specials methods for object detection during training. Bag-of-Freebies (BoF) - Methods which are used to train the model with no added cost during inference time. e.g - data augmentation, negative hard mining, focal loss, label smoothing, IoU based loss, Bag-of-Specials (BoS) - Plugin modules and post processing methods which only increase the inference cost by a small amount but can significantly improve the accuracy of object detection. e.g - techniques to enlarge receptive field (SPP, ASPP, FPB), introducin attention mechanism(SE, SAM), strengthening a feature integration capability (BiFPN, ASFF), post processing(soft NMS) They modifty current SOTA methods to make them more suitable for single GPU training. They breakdown object detetors to be built of the following main parts -\nInput - Image, Patches and Image pyramid Backbones - VGG16, ResNet, EfficientNet etc. Neck - Additional blocks - SPP, ASPP, RFB, SAM Path-aggregation blocks* - FPN, PAN, NAS-FPN, Fully connected FPN, SFAM Heads - Dense Prediction (one - stage) - RPN, SSD, YOLO, RetinaNEt, CornerNEt, FCOS Sparse Prediction (two - stage) - Faster RCNN, Mask RCNN, RepPoints Methodology # They first look at different models which can be used as a backbone for the object detector, in particular they look at the following 3 models Their objective is to fine the optimal balance among the input network resolution, convolutional layer number, the param number and the number of filters. They also note the fact that there are differences in performance when looking at the model backbones performance in a classification setting vs an object detection setting. The next thing they looked at was paramater aggregation with FPN, PAN, ASFF etc. They mention that a detection model requires the following - Higher input network size (resolution) - for detecting multiple small-sized objects More layers - For a higher receptive field to cover the increased size of input network. More paramters - for greater capacity of a model to detetct multiple objects of different sizes in a single image. They selected the CSPDarknet53 as the bacbone model due the fact that it has a higher input resolution than the CSPResNeXt50 and contains more paramaters, along these theoretical justifications, they also say that they conducted many additional experiments. They use the SPP block over the CSPDarknet53, since it significantly increases the receptive field, separates out the most significant context features and causes almost no reduction of the network operation speed. They use PANet as the method for the instead of the FPN. Selection of BoF and BoS - They introduced new methods of data augmentation mosaic(look at image below) and Self-Adversarial training(SAT). In SAT - The network alters the original image, executing an adversarial attack on itself. The network then makes predictions on the altered image. They select optiman hyperparams while applying genetic algos They modifed some exsiting methods to make theur design suitble for efficient training and detection - modified SAM, modified PAN, and Cross mini-Batch Normalization (CmBN) They modify SAM from spatial-wise attention to pointwise attention, and replace shortcut connection of PAN to concatenation. Final list of config and BoF and BoS\nBackbone - CSPDarknet53 Neck - SPP, PAN Head - YOLO V3 Final list of BoS and BoF -\nBag of Freebies (BoF) for backbone: CutMix and Mosaic data augmentation, DropBlock regularization, Class label smoothing. Bag of Specials (BoS) for backbone: Mish activation, Cross-stage partial connections (CSP), Multiinput weighted residual connections (MiWRC) Bag of Freebies (BoF) for detector: CIoU-loss,CmBN, DropBlock regularization, Mosaic data augmentation, Self-Adversarial Training, Eliminate grid sensitivity, Using multiple anchors for a single ground truth, Cosine annealing scheduler, Optimal hyperparameters, Random training shapes. Bag of Specials (BoS) for detector: Mish activation, SPP-blockSAM-block, PAN path-aggregation block, DIoU-NMS. ","date":"16 July 2023","externalUrl":null,"permalink":"/papernotes/yolo-v5/","section":"Paper Reading Notes","summary":"\u003cp\u003eThese are notes for my reading of the YOLO v5 paper. Check out the orignal paper at the link below\u003c/p\u003e","title":"YOLO V4","type":"papernotes"},{"content":"","date":"6 June 2023","externalUrl":null,"permalink":"/tags/clustering/","section":"Tags","summary":"","title":"Clustering","type":"tags"},{"content":" Motive # The motive of the following post is to take a look at how to cluster geospatial data using the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm.\nAs a use case for this post we will look at how we can find the most representative data points for each continent to test geographical variation in a dataset for a theoretical ML model.\nWe will use the SEN12TP dataset for demostation here.\nWhat is DBSCAN and why is it useful for geospatial clustering? # The DBSCAN algorithm is a popular density-based clustering algorithm used for identifying clusters and outliers in spatial data. It is particularly well-suited for geospatial data because it can handle data with irregularly shaped clusters and varying densities effectively, something that traditional algorithms such as k-means are not able to do.\nDBSCAN defines clusters as dense regions of data points separated by regions of lower density. The algorithm works by exploring the density connectivity of the data points. To learn more about how it works visit the following https://www.analyticsvidhya.com/blog/2020/09/how-dbscan-clustering-works/.\nThe advantages of DBSCAN for geospatial data include-\nRobustness to noise: DBSCAN can effectively handle outliers and noise points as they are considered separate from clusters. It does not assign them to any specific cluster and does not force them to belong to a cluster. Ability to detect clusters of arbitrary shape: DBSCAN can identify clusters with irregular shapes, making it suitable for geospatial data where clusters may have complex spatial distributions. Handling of varying cluster densities: DBSCAN can handle clusters of different densities. It can adapt to regions with high-density clusters as well as regions with low-density clusters. Our problem statement # Let\u0026rsquo;s say we have a hypothetical model for sentinel-1 to sentinel-2 image to image transalation that we want to test on this dataset.\nIn particlar we want to test the model for geographical variance in performance per continent.\nThe dataset contains too many points per continent and we need some way to reduce these to get the most representative points for each continent.\nWe use the DBSCAN algorithm to find N most representative points per continent.\nDownloading the dataset # The SEN12TP dataset contains paired set of imagery from the sentinel-1 and sentinel-2 satellite, but we won\u0026rsquo;t be using the imagery here, wplt.figure(figsize=(20, 20)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.stock_img()\nax.plot(gdf[\u0026ldquo;lon\u0026rdquo;], gdf[\u0026ldquo;lat\u0026rdquo;], \u0026ldquo;o\u0026rdquo;, color=\u0026ldquo;r\u0026rdquo;) plt.title(\u0026ldquo;SEN12TP dataset locations\u0026rdquo;)to get geolocations from all over the world.\nDownload the metadata for the dataset using the following command -\nwget https://raw.githubusercontent.com/Sambhav300899/blog_notebooks/main/clustering/dbscan/sen12tp-metadata.json Loading the dataset # We can use geopandas to directly load the dataset.\nimport geopandas as gpd gdf = gpd.read_file(\u0026#34;sen12tp-metadata.json\u0026#34;) The dataset is in a different CRS than WGS84, converting it to WGS84 format will allow us to look at it as lattitude and longitude values. The epsg code for WGS84 format is 4326. We can use geopandas to convert our data to the WGS84 projection.\nEach row of the geopandas dataframe represents one image location. We extract the latitude and longitude of the centroid of each image location.\n# convert to crs 4326 to get lat lon gdf = gdf.to_crs(epsg=4326) gdf = gdf[[\u0026#34;geometry\u0026#34;]] # get lat lon of centroids gdf[\u0026#34;centroid\u0026#34;] = gdf[\u0026#34;geometry\u0026#34;].centroid gdf[\u0026#34;lat\u0026#34;] = gdf[\u0026#34;centroid\u0026#34;].y gdf[\u0026#34;lon\u0026#34;] = gdf[\u0026#34;centroid\u0026#34;].x Lets see what all data we have in the dataframe now -\ngdf.head() geometry centroid lat lon 0 POLYGON ((-70.41461164636456 -34.46821038082358,\u0026hellip; POINT (-70.30 -34.55) -34.5596 -70.3071 1 POLYGON ((72.28825977983655 34.050558629310395,\u0026hellip; POINT (72.399 33.96) 33.9627 72.3993 2 POLYGON ((-8.424361551813695 41.53310978422038,\u0026hellip; POINT (-8.305 41.44) 41.4423 -8.3054 3 POLYGON ((6.724249816765044 46.5562534318746,\u0026hellip; POINT (6.85 46.46) 46.4688 6.85822 4 POLYGON ((-0.554267167211116 42.60345308513507,\u0026hellip; POINT (-0.43 42.51) 42.5108 -0.43611 Plotting all of the locations # We will use the cartopy library to plot a map of the earth and the locations of our datapoints on it.\nimport cartopy.crs as ccrs plt.figure(figsize=(20, 20)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.stock_img() ax.plot(gdf[\u0026#34;lon\u0026#34;], gdf[\u0026#34;lat\u0026#34;], \u0026#34;o\u0026#34;, color=\u0026#34;r\u0026#34;) plt.title(\u0026#34;SEN12TP dataset locations\u0026#34;) Reverese geocoding # We want to get N points per continent, it is important to know what continent each location belongs to before we do this. To do this we will use the reverse_geocoder library.\nimport reverse_geocoder as rg continent_dict = { \u0026#34;NA\u0026#34;: \u0026#34;North America\u0026#34;, \u0026#34;SA\u0026#34;: \u0026#34;South America\u0026#34;, \u0026#34;AS\u0026#34;: \u0026#34;Asia\u0026#34;, \u0026#34;AF\u0026#34;: \u0026#34;Africa\u0026#34;, \u0026#34;OC\u0026#34;: \u0026#34;Oceania\u0026#34;, \u0026#34;EU\u0026#34;: \u0026#34;Europe\u0026#34;, \u0026#34;AQ\u0026#34;: \u0026#34;Antarctica\u0026#34;, } gdf[\u0026#34;continent_code\u0026#34;] = rg.search(list(map(tuple, gdf[[\u0026#34;lat\u0026#34;, \u0026#34;lon\u0026#34;]].values))) gdf[\u0026#34;continent_code\u0026#34;] = gdf[\u0026#34;continent_code\u0026#34;].apply( lambda x: pc.country_alpha2_to_continent_code(x[\u0026#34;cc\u0026#34;]) ) gdf[\u0026#34;continent_name\u0026#34;] = gdf[\u0026#34;continent_code\u0026#34;].apply(lambda x: continent_dict[x]) Let\u0026rsquo;s plot the points again to confirm that we geocoded correctly.\nplt.figure(figsize=(20, 20)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.stock_img() for continent_name in gdf[\u0026#34;continent_name\u0026#34;].unique(): ax.plot( gdf[gdf[\u0026#34;continent_name\u0026#34;] == continent_name][\u0026#34;lon\u0026#34;], gdf[gdf[\u0026#34;continent_name\u0026#34;] == continent_name][\u0026#34;lat\u0026#34;], \u0026#34;o\u0026#34;, label=continent_name, ) plt.legend() plt.title(\u0026#34;SEN12TP dataset locations\u0026#34;) Running DBSCAN per continent # We will now run the DBSCAN algorithm per continent to get 50 representative points for the same.\nWe first define functions to run the DBSCAN algorithm -\ndef get_centermost_point(cluster): # This function is useful for getting the centerpoint of a cluster # We first extract the centroid of the cluster centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y) # we then calculate the distrances between all points and the centerpoint of the cluster and take the one with the least distance centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m) return tuple(centermost_point) def get_n_filtered_pts(gdf_dbscan, max_pts=10, max_dist_km=200): # This function runs dbscan on a geodataframe and retruns `max_pts` representative locations for it. # the max_dist_km is the maximum distance that a point can be from the center of the cluster # we will also convert all of our data to radians # we first extract the latitudes and longitudes in the form of a numpy array coords = gdf_dbscan[[\u0026#34;lat\u0026#34;, \u0026#34;lon\u0026#34;]].values # we then calculate the `epsilon` paramter which is converting our `max_dist_km` to radians # look at this to understadnd - https://stackoverflow.com/a/49212829 kms_per_radian = 6371.0088 epsilon = max_dist_km / kms_per_radian # Run DBSCAN db = DBSCAN( eps=epsilon, min_samples=1, algorithm=\u0026#34;ball_tree\u0026#34;, metric=\u0026#34;haversine\u0026#34; ).fit(np.radians(coords)) # get labels for each point in our dataset cluster_labels = db.labels_ # get the number of clusters by looking at the number of unique clusters num_clusters = len(set(cluster_labels)) # get the different points in each cluster clusters = pd.Series([coords[cluster_labels == n] for n in range(num_clusters)]) # get the centermost point in each cluster filtered_pts = list(clusters.map(lambda x: get_centermost_point(x))) # if we have points more than the required amount, randomly sample if len(filtered_pts) \u0026gt; max_pts: filtered_pts = random.sample(filtered_pts, max_pts) return filtered_pts We then use the following code to run DBSCAN per continent and create a geodataframe of the filtered data points.\nnum_pts_per_continent = 50 all_filtered_pts = [] # run per continent for continent_name in gdf[\u0026#34;continent_name\u0026#34;].unique(): continent_gdf = gdf[gdf[\u0026#34;continent_name\u0026#34;] == continent_name] all_filtered_pts.extend(get_n_filtered_pts(continent_gdf, num_pts_per_continent)) # get all lat and lons for all continents lats, lons = zip(*all_filtered_pts) rep_points = pd.DataFrame({\u0026#34;lon\u0026#34;: lons, \u0026#34;lat\u0026#34;: lats}) # extract all other information about these points from the original geodataframe filtered_gdf = rep_points.apply( lambda row: gdf[(gdf[\u0026#34;lat\u0026#34;] == row[\u0026#34;lat\u0026#34;]) \u0026amp; (gdf[\u0026#34;lon\u0026#34;] == row[\u0026#34;lon\u0026#34;])].iloc[0], axis=1, ) Let\u0026rsquo;s visualise this now to see the final datapoints we get.\nplt.figure(figsize=(20, 20)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.stock_img() for continent_name in filtered_gdf[\u0026#34;continent_name\u0026#34;].unique(): ax.plot( filtered_gdf[filtered_gdf[\u0026#34;continent_name\u0026#34;] == continent_name][\u0026#34;lon\u0026#34;], filtered_gdf[filtered_gdf[\u0026#34;continent_name\u0026#34;] == continent_name][\u0026#34;lat\u0026#34;], \u0026#34;o\u0026#34;, label=continent_name, ) plt.legend() plt.title(\u0026#34;SEN12TP dataset locations\u0026#34;) Conclusion # We see that we get a pretty good representation of the geograhical locations of our dataset in these points. We can now furthur use this to test our models geograhical performance.\nGithub Notebook ","date":"6 June 2023","externalUrl":null,"permalink":"/blog/dbscan-clustering/","section":"My Personal Blog","summary":"\u003ch2 class=\"relative group\"\u003eMotive \n    \u003cdiv id=\"motive\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\" href=\"#motive\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eThe motive of the following post is to take a look at how to cluster geospatial data using the \u003cstrong\u003eDensity-Based Spatial Clustering of Applications with Noise\u003c/strong\u003e (DBSCAN) algorithm.\u003c/p\u003e","title":"DBSCAN clustering and reverse geocoding for geospatial data","type":"blog"},{"content":"","date":"6 June 2023","externalUrl":null,"permalink":"/tags/visualization/","section":"Tags","summary":"","title":"Visualization","type":"tags"},{"content":"These are notes for my reading of the segment anything paper. Check out the orignal project page by Meta AI at the link below\nSegment Anything by Meta AI High Level # Main idea behind the model is to build a foundational model for Computer Vision similar to the state of the NLP community + allow it to be promptable. This model can then be used for downstream segmentation problems using prompt engineering and for some cases zero shot finetuning.\nThey mention 3 main parts of the project as -\nWhat task will enable zero-shot generalization?\nChose promptable segmentation, as it is a suitable pretraining task for a lot of CV problems.\nGoal of the task is to return a valid segmentation mask given any segmentation prompt.\nWhat is the corresponding model architecture?\nReal-time flexible promptable model\nFlexible here means it must be able give a valid segmentation mask for any prompt. Since the model is supposed to be interactive, it needs real time outputs. Model -\na powerful image encoder computes an image embedding\na prompt encoder embeds prompts\nImage an prompt embedding sources are combined in a lightweight mask decoder that predicts segmentation masks\nTypes of supported prompts -\nBox\nMask\nPoint\nFree form text\nWhat data can power this task and model?\nTo build the dataset, they use the model for labelling. They call this the \u0026ldquo;data engine\u0026rdquo;\nTo achieve strong generalisations to new data distributions, it was necessary to train SAM on large and diverse set of masks. No existing dataset satisfied these requirements.\nThey use \u0026lsquo;model in the loop\u0026rsquo; annotation.\n3 stages to the \u0026ldquo;data engine\u0026rdquo; - Assisted-manual\nModel assists annotators in annotation. Semi-auto.\nModel can generate annotations for a subset of objects by prompting with likely object locations and masks. (is this because of good performance after stage 1?)\nAnnotators annotate the rest of the objects.\nFully auto\nModel is prompted with a regular grid of foreground points. (what does this mean?)\nThis results in generation of ~100 high quality masks per image Final dataset - The final dataset, SA-1B, includes more than 1B masks from 11M licensed and privacy-preserving images. The masks were manually verified to have good annotations. Focused on AI ethics to look at potential geographical and racial biases the model might have\nExperiments\nTested using 23 segmentation datasets to find out that it generated high-quality masks from a single foreground point, these annotations were only slightly below the ground truth. Got good results on zeroshot downstream tasks such as edge detection, object proposal generation, instance segmentation and preliminary exploration of text-to-mask prediction\nThese results suggest that SAM can be used out-of-the-box with prompt engineering to solve a variety of tasks involving object and image distributions beyond the model\u0026rsquo;s training data.\nTask # The promptable segmentation task is to return a valid segmentation mask given any prompt. “Valid” here means that a segmentation mask should be returned even if the prompt is ambiguous. Look at the figure below to understand what ambiguous means here. This task was chosen since it acts as a natural pre-training algorithm and a general method for zero-shot transfer to downstream tasks via prompting.\nOne suggested method of using for downstream instance segmentation\nFor example, if one has a bounding box detector for cats, cat instance segmentation can be solved by providing the detector’s box output as a prompt to our model. An important distinction pointed out by them is that the model is not trained in a multitask scenario but can perform different tasks at inference tasks by pairing it with different components.\nModel # Three main components - image encoder\nThey use masked autoencoder (MAE) pre-trained ViT adapted to process high res outputs. What is a masked autoencoder?\na flexible prompt encoder\nPrompts are broken down into 2 categories- Sparse - points, boxes and text Dense - mask\nPoints and boxes are encoded using positional embeddings Text is converted to embeddings using CLIP encoder.\nDense prompts are encoded using convolutional operations.\nfast mask decoder\nThe mask decoder efficiently maps the image embedding, prompt embeddings, and an output token to a mask.\nThe modified decoder block uses prompt self-attention and cross-attention in two directions (prompt-to-image embedding and vice-versa) to update all embeddings.\nAfter running two blocks, we upsample the image embedding and an MLP maps the output token to a dynamic linear classifier, which then computes the mask foreground probability at each image location\nThe model was modified to allow for multiple masks being generated for a single prompt. This is to tackle ambiguity in the prompts. They set the limit of masks generated to 3, as they found it to be sufficient for most usecases. The model is designed to be efficient. The image encoder is the computationally heavy part of the model, afterwards the prompt encoder and mask decoder can run efficiently even in a browser.\nThis design allows testing the model with different prompts very easily, since the image embeddings don\u0026rsquo;t need to be computed repeatedly. Mask prediction - dice + focal loss was used for training.\nData Engine # The data engine has three stages - model-assisted manual annotation stage\nNormal image annotation assisted by the model in an interactive browser UI environment\nNo named labels for the objects were collected.\nThe model was trained on common segmentation datasets before this and then on the slice of the data that had been annotated in stage 1.\n4.3M masks were collected from 120k images in this stage.\nModel was trained 5-6 times during this phase\nsemi-automatic stage with a mix of automatically predicted masks and model-assisted annotation The aim here was to increase the diversity of masks in order to improve the models ability to segment anything.\nThe model was allowed to make predictions first and then the less common objects were labelled by the annotators.\nModel was trained 5-6 times during this phase.\nfully automatic stage in which our model generates masks without annotator input\nEnough data had been collected by this point to allow for the model to train automatically.\nThe model was prompted with 32 x 32 grid of points and tasked to predict the masks for these.\nThe concept of stable mask is introduced, a mask is stable if thresholding the probability map at 0.5 - delta and 0.5 + delta results in similar masks. delta = 0.3\nmask_over = mask \u0026gt; 0.5 + delta\nmask_under = mask \u0026gt; 0.5 - delta\nsimilarity(mask_over, mask_under)\nAfter selecting confident masks NMS was applied to filter duplicates. Fully automatic mask generation was applied to 11M images to generated 1.1B masks.\nThe SA-1B dataset has been released online, it consists of only generated masks. A comparison of the generated masks was done with manually annotated masks and showed that the quality of the generated masks is good. The authors performed a bias and fairness analysis of the dataset as well. Zero-shot Experiments # They analysed the performance of the model on different zero-shot experiments. The tasks are as follows - Edge detection\nGenerated segmentation masks and removed duplicates using NMS. These were generated by prompting the model with 16x16 grids of foreground points. Edge maps are then computed using sobel filtering of un-thresholded mask probability maps and postprocessing.\nIt produced reasonable edge maps. Object proposal generation\nTo generate object proposals, they ran a slightly modified version of the automatic mask generation pipeline and output the masks as proposals. Instance segmentation\nSegment objects from free-form text\n","date":"28 May 2023","externalUrl":null,"permalink":"/papernotes/segment-anything/","section":"Paper Reading Notes","summary":"\u003cp\u003eThese are notes for my reading of the segment anything paper. Check out the orignal project page by Meta AI at the link below\u003c/p\u003e","title":"Segment Anything","type":"papernotes"},{"content":"","date":"28 May 2023","externalUrl":null,"permalink":"/tags/segmentation/","section":"Tags","summary":"","title":"Segmentation","type":"tags"},{"content":" Motive # Does looking at a table comparing model peformance across multiple metrics feel bothersome to you? Lets see how radar plots can help us quickly visually compare model performance across multiple metrics.\nThis is just a single use case of using radar plots though, these can be used for so much more such as - comparing multiple products across multiple metrics, comparing multiple people across multiple skills, etc.\nThe first time I appreciating radar plots was in Fifa 18, where you could compare players across pace, skill, strength, etc.very easily. Made quick substitutions very simple.\nVisually they look very cool and are certain to add visual flair to your next presentation.\nWhat are radar plots? # Defining them a bit more formally, a radar plot is a type of chart used to display multiple variables on a two-dimensional graph. Each variable is plotted on a separate axis that radiates from the center of the graph, and data points are connected to create a polygon shape.\nRadar plots are commonly used to compare the performance of different models or entities across multiple metrics. They are helpful for visualizing how different variables affect overall performance and are widely used in fields such as data analysis, engineering, and sports.\nSample data # Here we will just be using a randomly generated table of data. Lets say we have a hypothetical image to image translation problem. We will compare 3 models across 4 metrics SSIM, PSNR, MAE and MSE. We load this in the form of a pandas dataframe named sample_df.\nNOTE! Keep in mind the data makes no sense in terms of metrics and is only meant for visualisation purposes! Model SSIM MAE MSE PSNR Model_1 -0.1 2.1 4.70 32.6 Model_2 0.5 0.5 0.45 54.1 Model_3 0.9 0.3 0.12 15.4 Data Normalisation # The data needs to be normalized before we can plot it. This is because we are plotting multiple metrics on the same scale, so our values also need to be on the same scale.\nlets normalize all of our data to be between 0 and 1.\nSSIM varies b/w -1 and 1, to normalize we just add 1 and divide by 2. MAE can be vary between 0 and infinity, lets assume max valye is 3 for MAE. Same case as MAE, max value assumed to be 5 PSNR for 8-bit grayscale images typically varies b/w 0-60 dB, so here lets - just normalize by dividing by 60. sample_df[\u0026#34;SSIM\u0026#34;] = (sample_df[\u0026#34;SSIM\u0026#34;] + 1) / 2 sample_df[\u0026#34;MAE\u0026#34;] = sample_df[\u0026#34;MAE\u0026#34;] / 3 sample_df[\u0026#34;MSE\u0026#34;] = sample_df[\u0026#34;MSE\u0026#34;] / 5 sample_df[\u0026#34;PSNR\u0026#34;] = sample_df[\u0026#34;PSNR\u0026#34;] / 60 This is what the data looks like after normalization.\nModel SSIM MAE MSE PSNR Model_1 0.45 0.70 0.94 0.54 Model_2 0.75 0.16 0.09 0.90 Model_3 0.95 0.10 0.02 0.25 Plotting the data # We will use good ol\u0026rsquo; matplotlib to create our plots. The following function is used to create the plots. The different parts of the codes are explained in the comments.\ndef radar_plot(df, attrs_to_plot, color_list=[\u0026#34;r\u0026#34;, \u0026#34;g\u0026#34;, \u0026#34;b\u0026#34;], title=\u0026#34;Radar plot\u0026#34;): # get the number of different metrics to plot n_metrics = len(attrs_to_plot) # we get the angles for each different metric angles = list(np.linspace(0, 2 * np.pi, n_metrics, endpoint=False)) # get label for each metric labels = list(attrs_to_plot) # we need the plot to be closed, so we close the plot # by adding the first metric at the end again angles += angles[:1] labels += labels[:1] # pass polar as true for ciruclar plot fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True)) # Set the labels for each of the angles ax.set_thetagrids(np.degrees(angles), labels=labels) # since we normalized all of our data to be between 0 and 1 # we set the limits of the plot to be between 0 and 1 ax.set_ylim(0, 1) for color, index_name in zip(color_list, df.index): # get the values for each row of the metrics # do note that we set the model name as the index vals = list(df[attrs_to_plot].loc[index_name]) # wrap around the values so that the plot is closed vals += vals[:1] # plot the polygon ax.plot(angles, vals, linewidth=1, color=color, label=index_name) # fill the area with color ax.fill(angles, vals, alpha=0.25, color=color) plt.legend(loc=\u0026#34;upper right\u0026#34;, bbox_to_anchor=(1.3, 1), fontsize=11) plt.title(title, fontsize=14, pad=10) Do note that the index of the sample_df has been changed to be the model name instead of numeric values. We then call the radar_plot as follows -\nradar_plot( sample_df, attrs_to_plot=[\u0026#34;SSIM\u0026#34;, \u0026#34;MAE\u0026#34;, \u0026#34;MSE\u0026#34;, \u0026#34;PSNR\u0026#34;], title=\u0026#34;Radar plot for model comparison\u0026#34;) One of the issues we see here is that there is a mix of negatively(lower is better) and positively(higher is better) oriented scores. This can create a bit of confusion when looking at the plot. Maybe if we invert the MSE and MAE using(1 - metric_value) then it might make more sense?\nPlotting after inverting using the following code -\nsample_df[\u0026#34;MAE_rev\u0026#34;] = 1 - sample_df[\u0026#34;MAE\u0026#34;] sample_df[\u0026#34;MSE_rev\u0026#34;] = 1 - sample_df[\u0026#34;MSE\u0026#34;] radar_plot( sample_df, attrs_to_plot=[\u0026#34;SSIM\u0026#34;, \u0026#34;MAE_rev\u0026#34;, \u0026#34;MSE_rev\u0026#34;, \u0026#34;PSNR\u0026#34;], title=\u0026#34;Radar plot for model comparison\u0026#34;, ) Conclusions # Hmm\u0026hellip;, honestly still confusing. I think this can lead to confusion in explaining what rev_mae and rev_mse is. I think the better approach would be to plot negatively and positively oriented metrics on different plots altogether.\nBut regardless, these plots can be used for so much more than just comparing model metrics and can be used for comparing input features or data samples together. Here are some pros and cons of these plots.\nPros-\nGives a nice summary of the different metrics/features for different models/data samples etc. Way better than looking at a table and trying to identify what each model is best at. Radar plots are visually very easy to understand and look nice aesthetically. Can be adjusted very easily for multiple metrics. Cons -\nMore models/samples can result in visual clutter. e.g - plotting the performance of 10 models like this would look horrible. All data needs to be normalized before plotting. Mixing of positive and negative oriented metrics can lead to confusion. It is very hard to compare to radar plots if the range and variables are different. Github Notebook ","date":"9 May 2023","externalUrl":null,"permalink":"/blog/radar-plots/","section":"My Personal Blog","summary":"\u003c!-- Contents-\n- [Motive](#motive)\n- [What are radar plots?](#what-are-radar-plots)\n- [Sample data](#sample-data)\n- [Data Normalisation](#data-normalisation)\n- [Plotting the data](#plotting-the-data)\n- [Conclusions](#conclusions) --\u003e\n\n\u003ch2 class=\"relative group\"\u003eMotive \n    \u003cdiv id=\"motive\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\" href=\"#motive\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003eDoes looking at a table comparing model peformance across multiple metrics feel bothersome to you? Lets see how radar plots can help us quickly visually compare model performance across multiple metrics.\u003c/p\u003e","title":"Using Radar Plots to Compare Model Performance","type":"blog"},{"content":"","date":"9 May 2023","externalUrl":null,"permalink":"/tags/visualisation/","section":"Tags","summary":"","title":"Visualisation","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"}]