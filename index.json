[{"content":" What is this? #  This is my first journey of building barbershop AI, an app to try out different hairstyles virtually.\nI\u0026rsquo;ve wanted to work on this for a while now and buildspace\u0026rsquo;s nights \u0026amp; weekends S5 seems like the perfect platform to do so.\n","date":"1 July 2024","permalink":"/barbershop_ai/","section":"Building Barbershop AI","summary":"What is this? #  This is my first journey of building barbershop AI, an app to try out different hairstyles virtually.\nI\u0026rsquo;ve wanted to work on this for a while now and buildspace\u0026rsquo;s nights \u0026amp; weekends S5 seems like the perfect platform to do so.","title":"Building Barbershop AI"},{"content":"","date":"1 July 2024","permalink":"/","section":"Sambhav Singh Rohatgi","summary":"","title":"Sambhav Singh Rohatgi"},{"content":" What happened in week 2? #  Well, I was unfortunately sick for most of the week, but oh well. This week was meant for creating a toy of the product. The main things that I did were -\n Tried to look at some literature about virtual hairstyle tryon and see if there is anything I can use. Used one existing repo Barbershop (based on GAN inversion + blending) to try and build a toy using gradio for the UI. Carried out some tests with images to see what sort of results I could get! (find them in the sample results section)   Quick overview of the current pipeline #  This is just the general overview, for more details, I would suggest reading the paper!\n Initial thoughts #   The pipeline is slow, the embedding step specially takes the most time. It takes around ~7 mins to generate one result locally (with a RTX 3070) at 1024x1024. It\u0026rsquo;s annoying to play around and experiment with params due to this. Need to make this quicker for sure. It\u0026rsquo;s hard to make any assumptions about reasons as to conditions where the pipeline is going wrong due to the lack of samples having been generated with different settings. The alignment step done using the dlib face detector doesn\u0026rsquo;t work well at times to extract faces from the images. The facial area is cropped to only areas near the face, this results in the whole profile of the hair not being captured. This is a pretty big issue for images of women, since hair isn\u0026rsquo;t localised to only the facial zone. Pretty big issue! The process also ends up blending the facial structure a lot which is another issue. So pose + facial structure of reference image matters a lot for this pipeline. The results aren\u0026rsquo;t too great for the examples I tried out, maybe I need to experiment with the various params more though. Atleast provides a decent baseline for further work!   Some sample results #  ","date":"1 July 2024","permalink":"/barbershop_ai/week-2/","section":"Building Barbershop AI","summary":"What happened in week 2? #  Well, I was unfortunately sick for most of the week, but oh well. This week was meant for creating a toy of the product.","title":"Week 2 - Building a toy"},{"content":" What is this? #  This is my first week of building barbershop AI, an app to try out different hairstyles virtually. The motive of the first week was to select an idea and this is the one I landed on.\n Why? #  The reasons why I want to build this app are -\n Having gotten shitty haircuts. (you can say my haircut is garbage even now, don\u0026rsquo;t worry) Language barriers - Living as an expat in Paris, it\u0026rsquo;s been hard to explain what I want to barbers. Trying it out before its permanent - Reference pictures don\u0026rsquo;t fully allow me to imagine what a haircut looks like on me, if I could do that it would be great. Curiosity - This is an interesting problem for me from a theoretical point of view.   The Goal #   Start Track my progress and learnings. Have a working demo by the end of 6 weeks. See if my idea sparks any interest in others. ???? profit?  ","date":"24 June 2024","permalink":"/barbershop_ai/week-1/","section":"Building Barbershop AI","summary":"What is this? #  This is my first week of building barbershop AI, an app to try out different hairstyles virtually. The motive of the first week was to select an idea and this is the one I landed on.","title":"Week 1 - Select an idea"},{"content":"","date":"16 June 2024","permalink":"/tags/gcp/","section":"Tags","summary":"","title":"gcp"},{"content":"","date":"16 June 2024","permalink":"/series/mlflow-on-gcp/","section":"Series","summary":"","title":"MLflow on GCP"},{"content":"","date":"16 June 2024","permalink":"/tags/mlops/","section":"Tags","summary":"","title":"mlops"},{"content":"","date":"16 June 2024","permalink":"/blog/","section":"My Personal Blog","summary":"","title":"My Personal Blog"},{"content":" Motive #  MLflow is a popular library for experiment tracking for Machine Learning. In addition to experiment tracking, it also offers a model registery and tracking platform. MLFlow can be used with a local database and artifact store, this is usually good enough for running experiments personally. When we need multiple people to work on the same experiment or run training as a job, we need to host a remote MLflow server.\nIn this post we will look at how we can host a MLflow server on GCP. In part 1 of this 2 part series, we will first look at how to host the remote server with a VM.\nThe aim of this post is to create a reusable script, so that we don\u0026rsquo;t have to go through individual steps manually. We will only do the full script in the final post though, for now, we are going to use the UI where possible.\n Create backend DB #  We need to a cloud SQL database to store our data. This can be done by the following steps-\n Go to the cloud SQL page Click on create instance Click on create a PostgreSQL instance Fill in the instance ID and password. e.g. - I used \u0026ldquo;mlflow-backend-db\u0026rdquo; as the instance name and \u0026ldquo;test\u0026rdquo; as the password for now. When creating an DB instance, GCP creates the admin user as \u0026ldquo;postgres\u0026rdquo; by default, the entered password is linked to this admin username. For now select DB params and configuration to minimise cost, for example, I selected a custom config of 1 vCPU and 3.75 GB RAM. Also set storage capacity to 10 GB and make it single region. Under the connections settings, make it so that private IP is ticked. Under associated networking, select the default network. You can leave the rest as is. Click on create instance.  The instance will take some time to get created. A default database named \u0026ldquo;postgres\u0026rdquo; already exists, create a new db named \u0026ldquo;mlflow\u0026rdquo; (change the name according to your needs). To do this, go to the Databases section after selecting your instance and click on create database.  Create GCS bucket #  Create a GCS bucket to store our model training artifacts. You can use the following command -\ngsutil mb gs://{ARTIFACT_BUCKET_NAME} I used the bucket name \u0026ldquo;mlflow-sample-bucket\u0026rdquo;. Keep in mind GCS bucket names are globally unique, so you will need to enter a different name.\n Create VM instance #  To create a VM instance for our use -\n Go to the VM instance page Click on create instance Select an E2 instance. Under boot disk, click on change. Here select the \u0026ldquo;Deep Learning for Linux\u0026rdquo; OS. Under Identity and API access select Allow full access to all Cloud APIs. Under advanced_options/networking, add a \u0026ldquo;mlflow\u0026rdquo; network tag. This will be useful to create a firewall and apply it to only VM\u0026rsquo;s with this tag.  Click on create.   Add firewall rule #  We need to add a firewall rule to allow incoming connections when we run the server.\ngcloud compute --project={your-project-id} \\ firewall-rules create mlflow \\ --direction=INGRESS \\ --priority=1000 \\ --network=default \\ --action=ALLOW \\ --rules=tcp:5000 \\ --source-ranges=0.0.0.0/0 \\ --target-tags=mlflow We set the port to allow these connections to 5000, as that is the port mlflow uses by default. For the target-tags argument, we pass the mlflow tag, since that is the tag we assigned to our VM. You can also go and do this through the UI. The source range 0.0.0.0/0 allows anyone on the internet to access this.\n Run the MLflow server on the VM #  To run the MLflow server on the VM, connect to your VM using ssh and run the following commands-\npip install mlflow psycopg2-binary The next command is of the form\nmlflow server \\ -h 0.0.0.0 \\ -p 5000 \\ --backend-store-uri postgresql://{DB_USER_NAME}:{DB_USER_PASSOWRD}@{DB_PRIVATE_IP}:5432/mlflow \\ --default-artifact-root gs://{ARTIFACT_BUCKET_NAME} From our previous commands, we know the following -\nDB_USER_NAME=postgres (remember this is the default admin name) DB_USER_PASSWORD=test DB_PRIVATE_IP=can be found in the overview page of the DB ARTIFACT_BUCKET_NAME=mlflow-sample-bucket mlflow server \\ -h 0.0.0.0 \\ -p 5000 \\ --backend-store-uri postgresql://postgres:test@{private-ip}:5432/mlflow \\ --default-artifact-root gs://mlflow-sample-bucket We specify -h as 0.0.0.0 to listen on all available network interfaces, we specify -p as 5000 for the port to listen on. The 5432 port is the default port used by the DB here.\n Access the MLflow dashboard #  You should now be able to use the mlflow server by accessing port 5000 of the external IP of your VM instance. This can be found in the VM instances page.\nType the following in your browser address bar-\n{externel-ip}:5000 et voila!  Running a sample experiment #  So how can we actually include this in our code when running an experiment? It is very simple, just add a line to include the address of your tracking and you\u0026rsquo;re good. Look at the following very simple example for how to add it, other than that your code remains unchanged.\nSidenote, ensure that the instance you run this on (local or VM) has a GCP account with access to write to the bucket we created above. If it does and you are having authentication issues, run the following (usually solves default authentication issue for me, but you might need to debug further) -\ngcloud auth login gcloud auth application-default login import mlflow import mlflow.sklearn from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression import numpy as np from sklearn.metrics import mean_squared_error, r2_score # Generate random data np.random.seed(42) X = np.random.rand(100, 1) y = 3 * X.squeeze() + 2 + np.random.randn(100) * 0.5 ############## Set MLflow tracking URI ############## mlflow.set_tracking_uri(\u0026#34;http://\u0026lt;vm-external-ip\u0026gt;:5000\u0026#34;) ##################################################### # Split the data X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42 ) # Start an MLflow run with mlflow.start_run(): model = LinearRegression() model.fit(X_train, y_train) predictions = model.predict(X_test) mlflow.log_param(\u0026#34;fit_intercept\u0026#34;, model.fit_intercept) mse = mean_squared_error(y_test, predictions) r2 = r2_score(y_test, predictions) mlflow.log_metric(\u0026#34;mse\u0026#34;, mse) mlflow.log_metric(\u0026#34;r2\u0026#34;, r2) mlflow.sklearn.log_model(model, \u0026#34;linear_regression_model\u0026#34;) print(f\u0026#34;Model saved in run {mlflow.active_run().info.run_id}\u0026#34;) print(f\u0026#34;MSE: {mse}\u0026#34;) print(f\u0026#34;R2: {r2}\u0026#34;) After this your run should be visible on your mlflow dashboard (\u0026lt;external-ip\u0026gt;:5000) and would look something like the following -  Issues with current approach #  So we got this working, but there are many problems with this implementation, such as -\n We have no authentication, anyone can access this dashboard, not such a great approach. We have no set version of mlflow being used, it might make more sense to use a more containerised approach. Scaling would be an issue, we will manually have to manage the VM. We also pay for idle resources during low traffic.  In part 2, we will try to find an approach that solves this issue.\n","date":"16 June 2024","permalink":"/blog/mlflow-on-gcp-1/","section":"My Personal Blog","summary":"Motive #  MLflow is a popular library for experiment tracking for Machine Learning. In addition to experiment tracking, it also offers a model registery and tracking platform. MLFlow can be used with a local database and artifact store, this is usually good enough for running experiments personally.","title":"Part 1: Hosting your custom MLflow server on GCP VM"},{"content":"","date":"16 June 2024","permalink":"/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"16 June 2024","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"16 July 2023","permalink":"/tags/object-detection/","section":"Tags","summary":"","title":"object detection"},{"content":"","date":"16 July 2023","permalink":"/tags/paper-notes/","section":"Tags","summary":"","title":"paper notes"},{"content":"This section contains notes from papers I have read, these can be very unrefined and unedited.\n","date":"16 July 2023","permalink":"/papernotes/","section":"Paper Reading Notes","summary":"This section contains notes from papers I have read, these can be very unrefined and unedited.","title":"Paper Reading Notes"},{"content":"These are notes for my reading of the YOLO v5 paper. Check out the orignal paper at the link below\nYOLO v4    --  High Level #    They create a CNN that operates in real time on a conventional GPU and can be trained on a single GPU as well.\n  Their main contributions are as follows -\n Develop and efficient and powerful object detection model. It means conventional GPUs such as a 1080 Ti or 2080 Ti can be used to train the model. They look at the SOTA Bag-of-Freebies and Bag-of-Specials methods for object detection during training. Bag-of-Freebies (BoF) - Methods which are used to train the model with no added cost during inference time. e.g - data augmentation, negative hard mining, focal loss, label smoothing, IoU based loss, Bag-of-Specials (BoS) - Plugin modules and post processing methods which only increase the inference cost by a small amount but can significantly improve the accuracy of object detection. e.g - techniques to enlarge receptive field (SPP, ASPP, FPB), introducin attention mechanism(SE, SAM), strengthening a feature integration capability (BiFPN, ASFF), post processing(soft NMS) They modifty current SOTA methods to make them more suitable for single GPU training.    They breakdown object detetors to be built of the following main parts -\n Input - Image, Patches and Image pyramid Backbones - VGG16, ResNet, EfficientNet etc. Neck -  Additional blocks - SPP, ASPP, RFB, SAM Path-aggregation blocks* - FPN, PAN, NAS-FPN, Fully connected FPN, SFAM   Heads -  Dense Prediction (one - stage) - RPN, SSD, YOLO, RetinaNEt, CornerNEt, FCOS Sparse Prediction (two - stage) - Faster RCNN, Mask RCNN, RepPoints       Methodology #   They first look at different models which can be used as a backbone for the object detector, in particular they look at the following 3 models  --  Their objective is to fine the optimal balance among the input network resolution, convolutional layer number, the param number and the number of filters. They also note the fact that there are differences in performance when looking at the model backbones performance in a classification setting vs an object detection setting. The next thing they looked at was paramater aggregation with FPN, PAN, ASFF etc. They mention that a detection model requires the following -  Higher input network size (resolution) - for detecting multiple small-sized objects More layers - For a higher receptive field to cover the increased size of input network. More paramters - for greater capacity of a model to detetct multiple objects of different sizes in a single image.   They selected the CSPDarknet53 as the bacbone model due the fact that it has a higher input resolution than the CSPResNeXt50 and contains more paramaters, along these theoretical justifications, they also say that they conducted many additional experiments. They use the SPP block over the CSPDarknet53, since it significantly increases the receptive field, separates out the most significant context features and causes almost no reduction of the network operation speed. They use PANet as the method for the instead of the FPN.  Selection of BoF and BoS -  They introduced new methods of data augmentation mosaic(look at image below) and Self-Adversarial training(SAT). In SAT -  The network alters the original image, executing an adversarial attack on itself. The network then makes predictions on the altered image.   They select optiman hyperparams while applying genetic algos They modifed some exsiting methods to make theur design suitble for efficient training and detection - modified SAM, modified PAN, and Cross mini-Batch Normalization (CmBN)    --  They modify SAM from spatial-wise attention to pointwise attention, and replace shortcut connection of PAN to concatenation.  --   Final list of config and BoF and BoS\n Backbone - CSPDarknet53 Neck - SPP, PAN Head - YOLO V3    Final list of BoS and BoF -\n Bag of Freebies (BoF) for backbone: CutMix and Mosaic data augmentation, DropBlock regularization, Class label smoothing. Bag of Specials (BoS) for backbone: Mish activation, Cross-stage partial connections (CSP), Multiinput weighted residual connections (MiWRC) Bag of Freebies (BoF) for detector: CIoU-loss,CmBN, DropBlock regularization, Mosaic data augmentation, Self-Adversarial Training, Eliminate grid sensitivity, Using multiple anchors for a single ground truth, Cosine annealing scheduler, Optimal hyperparameters, Random training shapes. Bag of Specials (BoS) for detector: Mish activation, SPP-blockSAM-block, PAN path-aggregation block, DIoU-NMS.    ","date":"16 July 2023","permalink":"/papernotes/yolo-v5/","section":"Paper Reading Notes","summary":"These are notes for my reading of the YOLO v5 paper. Check out the orignal paper at the link below\nYOLO v4    --  High Level #    They create a CNN that operates in real time on a conventional GPU and can be trained on a single GPU as well.","title":"YOLO V4"},{"content":"","date":"6 June 2023","permalink":"/tags/clustering/","section":"Tags","summary":"","title":"clustering"},{"content":" Motive #  The motive of the following post is to take a look at how to cluster geospatial data using the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm.\nAs a use case for this post we will look at how we can find the most representative data points for each continent to test geographical variation in a dataset for a theoretical ML model.\nWe will use the SEN12TP dataset for demostation here.\n What is DBSCAN and why is it useful for geospatial clustering? #  The DBSCAN algorithm is a popular density-based clustering algorithm used for identifying clusters and outliers in spatial data. It is particularly well-suited for geospatial data because it can handle data with irregularly shaped clusters and varying densities effectively, something that traditional algorithms such as k-means are not able to do.\nDBSCAN defines clusters as dense regions of data points separated by regions of lower density. The algorithm works by exploring the density connectivity of the data points. To learn more about how it works visit the following https://www.analyticsvidhya.com/blog/2020/09/how-dbscan-clustering-works/.\nThe advantages of DBSCAN for geospatial data include-\n Robustness to noise: DBSCAN can effectively handle outliers and noise points as they are considered separate from clusters. It does not assign them to any specific cluster and does not force them to belong to a cluster. Ability to detect clusters of arbitrary shape: DBSCAN can identify clusters with irregular shapes, making it suitable for geospatial data where clusters may have complex spatial distributions. Handling of varying cluster densities: DBSCAN can handle clusters of different densities. It can adapt to regions with high-density clusters as well as regions with low-density clusters.   Our problem statement #    Let\u0026rsquo;s say we have a hypothetical model for sentinel-1 to sentinel-2 image to image transalation that we want to test on this dataset.\n  In particlar we want to test the model for geographical variance in performance per continent.\n  The dataset contains too many points per continent and we need some way to reduce these to get the most representative points for each continent.\n  We use the DBSCAN algorithm to find N most representative points per continent.\n   Downloading the dataset #  The SEN12TP dataset contains paired set of imagery from the sentinel-1 and sentinel-2 satellite, but we won\u0026rsquo;t be using the imagery here, wplt.figure(figsize=(20, 20)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.stock_img()\nax.plot(gdf[\u0026ldquo;lon\u0026rdquo;], gdf[\u0026ldquo;lat\u0026rdquo;], \u0026ldquo;o\u0026rdquo;, color=\u0026ldquo;r\u0026rdquo;) plt.title(\u0026ldquo;SEN12TP dataset locations\u0026rdquo;)to get geolocations from all over the world.\nDownload the metadata for the dataset using the following command -\nwget https://raw.githubusercontent.com/Sambhav300899/blog_notebooks/main/clustering/dbscan/sen12tp-metadata.json  Loading the dataset #  We can use geopandas to directly load the dataset.\nimport geopandas as gpd gdf = gpd.read_file(\u0026#34;sen12tp-metadata.json\u0026#34;) The dataset is in a different CRS than WGS84, converting it to WGS84 format will allow us to look at it as lattitude and longitude values. The epsg code for WGS84 format is 4326. We can use geopandas to convert our data to the WGS84 projection.\nEach row of the geopandas dataframe represents one image location. We extract the latitude and longitude of the centroid of each image location.\n# convert to crs 4326 to get lat lon gdf = gdf.to_crs(epsg=4326) gdf = gdf[[\u0026#34;geometry\u0026#34;]] # get lat lon of centroids gdf[\u0026#34;centroid\u0026#34;] = gdf[\u0026#34;geometry\u0026#34;].centroid gdf[\u0026#34;lat\u0026#34;] = gdf[\u0026#34;centroid\u0026#34;].y gdf[\u0026#34;lon\u0026#34;] = gdf[\u0026#34;centroid\u0026#34;].x Lets see what all data we have in the dataframe now -\ngdf.head()     geometry centroid lat lon     0 POLYGON ((-70.41461164636456 -34.46821038082358,\u0026hellip; POINT (-70.30 -34.55) -34.5596 -70.3071   1 POLYGON ((72.28825977983655 34.050558629310395,\u0026hellip; POINT (72.399 33.96) 33.9627 72.3993   2 POLYGON ((-8.424361551813695 41.53310978422038,\u0026hellip; POINT (-8.305 41.44) 41.4423 -8.3054   3 POLYGON ((6.724249816765044 46.5562534318746,\u0026hellip; POINT (6.85 46.46) 46.4688 6.85822   4 POLYGON ((-0.554267167211116 42.60345308513507,\u0026hellip; POINT (-0.43 42.51) 42.5108 -0.43611     Plotting all of the locations #  We will use the cartopy library to plot a map of the earth and the locations of our datapoints on it.\nimport cartopy.crs as ccrs plt.figure(figsize=(20, 20)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.stock_img() ax.plot(gdf[\u0026#34;lon\u0026#34;], gdf[\u0026#34;lat\u0026#34;], \u0026#34;o\u0026#34;, color=\u0026#34;r\u0026#34;) plt.title(\u0026#34;SEN12TP dataset locations\u0026#34;)  Reverese geocoding #  We want to get N points per continent, it is important to know what continent each location belongs to before we do this. To do this we will use the reverse_geocoder library.\nimport reverse_geocoder as rg continent_dict = { \u0026#34;NA\u0026#34;: \u0026#34;North America\u0026#34;, \u0026#34;SA\u0026#34;: \u0026#34;South America\u0026#34;, \u0026#34;AS\u0026#34;: \u0026#34;Asia\u0026#34;, \u0026#34;AF\u0026#34;: \u0026#34;Africa\u0026#34;, \u0026#34;OC\u0026#34;: \u0026#34;Oceania\u0026#34;, \u0026#34;EU\u0026#34;: \u0026#34;Europe\u0026#34;, \u0026#34;AQ\u0026#34;: \u0026#34;Antarctica\u0026#34;, } gdf[\u0026#34;continent_code\u0026#34;] = rg.search(list(map(tuple, gdf[[\u0026#34;lat\u0026#34;, \u0026#34;lon\u0026#34;]].values))) gdf[\u0026#34;continent_code\u0026#34;] = gdf[\u0026#34;continent_code\u0026#34;].apply( lambda x: pc.country_alpha2_to_continent_code(x[\u0026#34;cc\u0026#34;]) ) gdf[\u0026#34;continent_name\u0026#34;] = gdf[\u0026#34;continent_code\u0026#34;].apply(lambda x: continent_dict[x]) Let\u0026rsquo;s plot the points again to confirm that we geocoded correctly.\nplt.figure(figsize=(20, 20)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.stock_img() for continent_name in gdf[\u0026#34;continent_name\u0026#34;].unique(): ax.plot( gdf[gdf[\u0026#34;continent_name\u0026#34;] == continent_name][\u0026#34;lon\u0026#34;], gdf[gdf[\u0026#34;continent_name\u0026#34;] == continent_name][\u0026#34;lat\u0026#34;], \u0026#34;o\u0026#34;, label=continent_name, ) plt.legend() plt.title(\u0026#34;SEN12TP dataset locations\u0026#34;)  Running DBSCAN per continent #  We will now run the DBSCAN algorithm per continent to get 50 representative points for the same.\nWe first define functions to run the DBSCAN algorithm -\ndef get_centermost_point(cluster): # This function is useful for getting the centerpoint of a cluster # We first extract the centroid of the cluster centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y) # we then calculate the distrances between all points and the centerpoint of the cluster and take the one with the least distance centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m) return tuple(centermost_point) def get_n_filtered_pts(gdf_dbscan, max_pts=10, max_dist_km=200): # This function runs dbscan on a geodataframe and retruns `max_pts` representative locations for it.  # the max_dist_km is the maximum distance that a point can be from the center of the cluster # we will also convert all of our data to radians # we first extract the latitudes and longitudes in the form of a numpy array coords = gdf_dbscan[[\u0026#34;lat\u0026#34;, \u0026#34;lon\u0026#34;]].values # we then calculate the `epsilon` paramter which is converting our `max_dist_km` to radians # look at this to understadnd - https://stackoverflow.com/a/49212829 kms_per_radian = 6371.0088 epsilon = max_dist_km / kms_per_radian # Run DBSCAN db = DBSCAN( eps=epsilon, min_samples=1, algorithm=\u0026#34;ball_tree\u0026#34;, metric=\u0026#34;haversine\u0026#34; ).fit(np.radians(coords)) # get labels for each point in our dataset cluster_labels = db.labels_ # get the number of clusters by looking at the number of unique clusters num_clusters = len(set(cluster_labels)) # get the different points in each cluster clusters = pd.Series([coords[cluster_labels == n] for n in range(num_clusters)]) # get the centermost point in each cluster filtered_pts = list(clusters.map(lambda x: get_centermost_point(x))) # if we have points more than the required amount, randomly sample if len(filtered_pts) \u0026gt; max_pts: filtered_pts = random.sample(filtered_pts, max_pts) return filtered_pts We then use the following code to run DBSCAN per continent and create a geodataframe of the filtered data points.\nnum_pts_per_continent = 50 all_filtered_pts = [] # run per continent for continent_name in gdf[\u0026#34;continent_name\u0026#34;].unique(): continent_gdf = gdf[gdf[\u0026#34;continent_name\u0026#34;] == continent_name] all_filtered_pts.extend(get_n_filtered_pts(continent_gdf, num_pts_per_continent)) # get all lat and lons for all continents lats, lons = zip(*all_filtered_pts) rep_points = pd.DataFrame({\u0026#34;lon\u0026#34;: lons, \u0026#34;lat\u0026#34;: lats}) # extract all other information about these points from the original geodataframe filtered_gdf = rep_points.apply( lambda row: gdf[(gdf[\u0026#34;lat\u0026#34;] == row[\u0026#34;lat\u0026#34;]) \u0026amp; (gdf[\u0026#34;lon\u0026#34;] == row[\u0026#34;lon\u0026#34;])].iloc[0], axis=1, ) Let\u0026rsquo;s visualise this now to see the final datapoints we get.\nplt.figure(figsize=(20, 20)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.stock_img() for continent_name in filtered_gdf[\u0026#34;continent_name\u0026#34;].unique(): ax.plot( filtered_gdf[filtered_gdf[\u0026#34;continent_name\u0026#34;] == continent_name][\u0026#34;lon\u0026#34;], filtered_gdf[filtered_gdf[\u0026#34;continent_name\u0026#34;] == continent_name][\u0026#34;lat\u0026#34;], \u0026#34;o\u0026#34;, label=continent_name, ) plt.legend() plt.title(\u0026#34;SEN12TP dataset locations\u0026#34;)  Conclusion #  We see that we get a pretty good representation of the geograhical locations of our dataset in these points. We can now furthur use this to test our models geograhical performance.\n  Github Notebook  ","date":"6 June 2023","permalink":"/blog/dbscan-clustering/","section":"My Personal Blog","summary":"Motive #  The motive of the following post is to take a look at how to cluster geospatial data using the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm.","title":"DBSCAN clustering and reverse geocoding for geospatial data"},{"content":"","date":"6 June 2023","permalink":"/tags/geospatial/","section":"Tags","summary":"","title":"geospatial"},{"content":"","date":"6 June 2023","permalink":"/tags/visualization/","section":"Tags","summary":"","title":"visualization"},{"content":"These are notes for my reading of the segment anything paper. Check out the orignal project page by Meta AI at the link below\nSegment Anything by Meta AI     High Level #    Main idea behind the model is to build a foundational model for Computer Vision similar to the state of the NLP community + allow it to be promptable.   This model can then be used for downstream segmentation problems using prompt engineering and for some cases zero shot finetuning.\n  They mention 3 main parts of the project as -\n  What task will enable zero-shot generalization?\n  Chose promptable segmentation, as it is a suitable pretraining task for a lot of CV problems.\n  Goal of the task is to return a valid segmentation mask given any segmentation prompt.\n    What is the corresponding model architecture?\n  Real-time flexible promptable model\n  Flexible here means it must be able give a valid segmentation mask for any prompt.   Since the model is supposed to be interactive, it needs real time outputs.   Model -\n  a powerful image encoder computes an image embedding\n  a prompt encoder embeds prompts\n  Image an prompt embedding sources are combined in a lightweight mask decoder that predicts segmentation masks\n    Types of supported prompts -\n  Box\n  Mask\n  Point\n  Free form text\n    What data can power this task and model?\n  To build the dataset, they use the model for labelling. They call this the \u0026ldquo;data engine\u0026rdquo;\n  To achieve strong generalisations to new data distributions, it was necessary to train SAM on large and diverse set of masks.   No existing dataset satisfied these requirements.\n  They use \u0026lsquo;model in the loop\u0026rsquo; annotation.\n  3 stages to the \u0026ldquo;data engine\u0026rdquo; -   Assisted-manual\n Model assists annotators in annotation.     Semi-auto.\n  Model can generate annotations for a subset of objects by prompting with likely object locations and masks. (is this because of good performance after stage 1?)\n  Annotators annotate the rest of the objects.\n    Fully auto\n  Model is prompted with a regular grid of foreground points. (what does this mean?)\n  This results in generation of ~100 high quality masks per image             Final dataset - The final dataset, SA-1B, includes more than 1B masks from 11M licensed and privacy-preserving images. The masks were manually verified to have good annotations.   Focused on AI ethics to look at potential geographical and racial biases the model might have\n  Experiments\n  Tested using 23 segmentation datasets to find out that it generated high-quality masks from a single foreground point, these annotations were only slightly below the ground truth.   Got good results on zeroshot downstream tasks such as edge detection, object proposal generation, instance segmentation and preliminary exploration of text-to-mask prediction\n  These results suggest that SAM can be used out-of-the-box with prompt engineering to solve a variety of tasks involving object and image distributions beyond the model\u0026rsquo;s training data.\n     Task #   The promptable segmentation task is to return a valid segmentation mask given any prompt. “Valid” here means that a segmentation mask should be returned even if the prompt is ambiguous. Look at the figure below to understand what ambiguous means here.    This task was chosen since it acts as a natural pre-training algorithm and a general method for zero-shot transfer to downstream tasks via prompting.\n  One suggested method of using for downstream instance segmentation\n For example, if one has a bounding box detector for cats, cat instance segmentation can be solved by providing the detector’s box output as a prompt to our model.    An important distinction pointed out by them is that the model is not trained in a multitask scenario but can perform different tasks at inference tasks by pairing it with different components.\n   Model #    Three main components -   image encoder\n  They use masked autoencoder (MAE) pre-trained ViT adapted to process high res outputs.   What is a masked autoencoder?\n    a flexible prompt encoder\n  Prompts are broken down into 2 categories-   Sparse - points, boxes and text   Dense - mask\n  Points and boxes are encoded using positional embeddings   Text is converted to embeddings using CLIP encoder.\n  Dense prompts are encoded using convolutional operations.\n    fast mask decoder\n  The mask decoder efficiently maps the image embedding, prompt embeddings, and an output token to a mask.\n  The modified decoder block uses prompt self-attention and cross-attention in two directions (prompt-to-image embedding and vice-versa) to update all embeddings.\n  After running two blocks, we upsample the image embedding and an MLP maps the output token to a dynamic linear classifier, which then computes the mask foreground probability at each image location\n      The model was modified to allow for multiple masks being generated for a single prompt. This is to tackle ambiguity in the prompts. They set the limit of masks generated to 3, as they found it to be sufficient for most usecases.   The model is designed to be efficient. The image encoder is the computationally heavy part of the model, afterwards the prompt encoder and mask decoder can run efficiently even in a browser.\n  This design allows testing the model with different prompts very easily, since the image embeddings don\u0026rsquo;t need to be computed repeatedly.   Mask prediction - dice + focal loss was used for training.\n   Data Engine #    The data engine has three stages -   model-assisted manual annotation stage\n  Normal image annotation assisted by the model in an interactive browser UI environment\n  No named labels for the objects were collected.\n  The model was trained on common segmentation datasets before this and then on the slice of the data that had been annotated in stage 1.\n  4.3M masks were collected from 120k images in this stage.\n  Model was trained 5-6 times during this phase\n    semi-automatic stage with a mix of automatically predicted masks and model-assisted annotation   The aim here was to increase the diversity of masks in order to improve the models ability to segment anything.\n  The model was allowed to make predictions first and then the less common objects were labelled by the annotators.\n  Model was trained 5-6 times during this phase.\n    fully automatic stage in which our model generates masks without annotator input\n  Enough data had been collected by this point to allow for the model to train automatically.\n  The model was prompted with 32 x 32 grid of points and tasked to predict the masks for these.\n  The concept of stable mask is introduced, a mask is stable if thresholding the probability map at 0.5 - delta and 0.5 + delta results in similar masks.   delta = 0.3\n  mask_over = mask \u0026gt; 0.5 + delta\n  mask_under = mask \u0026gt; 0.5 - delta\n  similarity(mask_over, mask_under)\n    After selecting confident masks NMS was applied to filter duplicates.   Fully automatic mask generation was applied to 11M images to generated 1.1B masks.\n      The SA-1B dataset has been released online, it consists of only generated masks.   A comparison of the generated masks was done with manually annotated masks and showed that the quality of the generated masks is good.   The authors performed a bias and fairness analysis of the dataset as well.    Zero-shot Experiments #    They analysed the performance of the model on different zero-shot experiments.   The tasks are as follows -   Edge detection\n  Generated segmentation masks and removed duplicates using NMS. These were generated by prompting the model with 16x16 grids of foreground points.   Edge maps are then computed using sobel filtering of un-thresholded mask probability maps and postprocessing.\n  It produced reasonable edge maps.     Object proposal generation\n To generate object proposals, they ran a slightly modified version of the automatic mask generation pipeline and output the masks as proposals.    Instance segmentation\n  Segment objects from free-form text\n    ","date":"28 May 2023","permalink":"/papernotes/segment-anything/","section":"Paper Reading Notes","summary":"These are notes for my reading of the segment anything paper. Check out the orignal project page by Meta AI at the link below\nSegment Anything by Meta AI     High Level #    Main idea behind the model is to build a foundational model for Computer Vision similar to the state of the NLP community + allow it to be promptable.","title":"Segment Anything"},{"content":"","date":"28 May 2023","permalink":"/tags/segmentation/","section":"Tags","summary":"","title":"segmentation"},{"content":" Motive #  Does looking at a table comparing model peformance across multiple metrics feel bothersome to you? Lets see how radar plots can help us quickly visually compare model performance across multiple metrics.\nThis is just a single use case of using radar plots though, these can be used for so much more such as - comparing multiple products across multiple metrics, comparing multiple people across multiple skills, etc.\nThe first time I appreciating radar plots was in Fifa 18, where you could compare players across pace, skill, strength, etc.very easily. Made quick substitutions very simple.\nVisually they look very cool and are certain to add visual flair to your next presentation.\n What are radar plots? #  Defining them a bit more formally, a radar plot is a type of chart used to display multiple variables on a two-dimensional graph. Each variable is plotted on a separate axis that radiates from the center of the graph, and data points are connected to create a polygon shape.\nRadar plots are commonly used to compare the performance of different models or entities across multiple metrics. They are helpful for visualizing how different variables affect overall performance and are widely used in fields such as data analysis, engineering, and sports.\n Sample data #  Here we will just be using a randomly generated table of data. Lets say we have a hypothetical image to image translation problem. We will compare 3 models across 4 metrics SSIM, PSNR, MAE and MSE. We load this in the form of a pandas dataframe named sample_df.\n   NOTE! Keep in mind the data makes no sense in terms of metrics and is only meant for visualisation purposes!     Model SSIM MAE MSE PSNR     Model_1 -0.1 2.1 4.70 32.6   Model_2 0.5 0.5 0.45 54.1   Model_3 0.9 0.3 0.12 15.4     Data Normalisation #  The data needs to be normalized before we can plot it. This is because we are plotting multiple metrics on the same scale, so our values also need to be on the same scale.\nlets normalize all of our data to be between 0 and 1.\n SSIM varies b/w -1 and 1, to normalize we just add 1 and divide by 2. MAE can be vary between 0 and infinity, lets assume max valye is 3 for MAE. Same case as MAE, max value assumed to be 5 PSNR for 8-bit grayscale images typically varies b/w 0-60 dB, so here lets - just normalize by dividing by 60.  sample_df[\u0026quot;SSIM\u0026quot;] = (sample_df[\u0026quot;SSIM\u0026quot;] + 1) / 2 sample_df[\u0026quot;MAE\u0026quot;] = sample_df[\u0026quot;MAE\u0026quot;] / 3 sample_df[\u0026quot;MSE\u0026quot;] = sample_df[\u0026quot;MSE\u0026quot;] / 5 sample_df[\u0026quot;PSNR\u0026quot;] = sample_df[\u0026quot;PSNR\u0026quot;] / 60 This is what the data looks like after normalization.\n   Model SSIM MAE MSE PSNR     Model_1 0.45 0.70 0.94 0.54   Model_2 0.75 0.16 0.09 0.90   Model_3 0.95 0.10 0.02 0.25     Plotting the data #  We will use good ol' matplotlib to create our plots. The following function is used to create the plots. The different parts of the codes are explained in the comments.\ndef radar_plot(df, attrs_to_plot, color_list=[\u0026#34;r\u0026#34;, \u0026#34;g\u0026#34;, \u0026#34;b\u0026#34;], title=\u0026#34;Radar plot\u0026#34;): # get the number of different metrics to plot n_metrics = len(attrs_to_plot) # we get the angles for each different metric angles = list(np.linspace(0, 2 * np.pi, n_metrics, endpoint=False)) # get label for each metric labels = list(attrs_to_plot) # we need the plot to be closed, so we close the plot # by adding the first metric at the end again angles += angles[:1] labels += labels[:1] # pass polar as true for ciruclar plot fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True)) # Set the labels for each of the angles ax.set_thetagrids(np.degrees(angles), labels=labels) # since we normalized all of our data to be between 0 and 1 # we set the limits of the plot to be between 0 and 1 ax.set_ylim(0, 1) for color, index_name in zip(color_list, df.index): # get the values for each row of the metrics # do note that we set the model name as the index vals = list(df[attrs_to_plot].loc[index_name]) # wrap around the values so that the plot is closed vals += vals[:1] # plot the polygon ax.plot(angles, vals, linewidth=1, color=color, label=index_name) # fill the area with color ax.fill(angles, vals, alpha=0.25, color=color) plt.legend(loc=\u0026#34;upper right\u0026#34;, bbox_to_anchor=(1.3, 1), fontsize=11) plt.title(title, fontsize=14, pad=10) Do note that the index of the sample_df has been changed to be the model name instead of numeric values. We then call the radar_plot as follows -\nradar_plot( sample_df, attrs_to_plot=[\u0026#34;SSIM\u0026#34;, \u0026#34;MAE\u0026#34;, \u0026#34;MSE\u0026#34;, \u0026#34;PSNR\u0026#34;], title=\u0026#34;Radar plot for model comparison\u0026#34;) One of the issues we see here is that there is a mix of negatively(lower is better) and positively(higher is better) oriented scores. This can create a bit of confusion when looking at the plot. Maybe if we invert the MSE and MAE using(1 - metric_value) then it might make more sense?\nPlotting after inverting using the following code -\nsample_df[\u0026#34;MAE_rev\u0026#34;] = 1 - sample_df[\u0026#34;MAE\u0026#34;] sample_df[\u0026#34;MSE_rev\u0026#34;] = 1 - sample_df[\u0026#34;MSE\u0026#34;] radar_plot( sample_df, attrs_to_plot=[\u0026#34;SSIM\u0026#34;, \u0026#34;MAE_rev\u0026#34;, \u0026#34;MSE_rev\u0026#34;, \u0026#34;PSNR\u0026#34;], title=\u0026#34;Radar plot for model comparison\u0026#34;, )  Conclusions #  Hmm\u0026hellip;, honestly still confusing. I think this can lead to confusion in explaining what rev_mae and rev_mse is. I think the better approach would be to plot negatively and positively oriented metrics on different plots altogether.\nBut regardless, these plots can be used for so much more than just comparing model metrics and can be used for comparing input features or data samples together. Here are some pros and cons of these plots.\nPros-\n Gives a nice summary of the different metrics/features for different models/data samples etc. Way better than looking at a table and trying to identify what each model is best at. Radar plots are visually very easy to understand and look nice aesthetically. Can be adjusted very easily for multiple metrics.  Cons -\n More models/samples can result in visual clutter. e.g - plotting the performance of 10 models like this would look horrible. All data needs to be normalized before plotting. Mixing of positive and negative oriented metrics can lead to confusion. It is very hard to compare to radar plots if the range and variables are different.    Github Notebook  ","date":"9 May 2023","permalink":"/blog/radar-plots/","section":"My Personal Blog","summary":"Motive #  Does looking at a table comparing model peformance across multiple metrics feel bothersome to you? Lets see how radar plots can help us quickly visually compare model performance across multiple metrics.","title":"Using Radar Plots to Compare Model Performance"},{"content":"","date":"9 May 2023","permalink":"/tags/visualisation/","section":"Tags","summary":"","title":"visualisation"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]