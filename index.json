[{"content":"","date":"9 May 2023","permalink":"/blog/","section":"My Personal Blog","summary":"","title":"My Personal Blog"},{"content":"","date":"9 May 2023","permalink":"/","section":"Sambhav Singh Rohatgi","summary":"","title":"Sambhav Singh Rohatgi"},{"content":"","date":"9 May 2023","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":" Motive # Does looking at a table comparing model peformance across multiple metrics feel bothersome to you? Lets see how radar plots can help us quickly visually compare model performance across multiple metrics.\nThis is just a single use case of using radar plots though, these can be used for so much more such as - comparing multiple products across multiple metrics, comparing multiple people across multiple skills, etc.\nThe first time I appreciating radar plots was in Fifa 18, where you could compare players across pace, skill, strength, etc.very easily. Made quick substitutions very simple.\nVisually they look very cool and are certain to add visual flair to your next presentation.\nWhat are radar plots? # Defining them a bit more formally, a radar plot is a type of chart used to display multiple variables on a two-dimensional graph. Each variable is plotted on a separate axis that radiates from the center of the graph, and data points are connected to create a polygon shape.\nRadar plots are commonly used to compare the performance of different models or entities across multiple metrics. They are helpful for visualizing how different variables affect overall performance and are widely used in fields such as data analysis, engineering, and sports.\nSample data # Here we will just be using a randomly generated table of data. Lets say we have a hypothetical image to image translation problem. We will compare 3 models across 4 metrics SSIM, PSNR, MAE and MSE. We load this in the form of a pandas dataframe named sample_df.\nNOTE! Keep in mind the data makes no sense in terms of metrics and is only meant for visualisation purposes! Model SSIM MAE MSE PSNR Model_1 -0.1 2.1 4.70 32.6 Model_2 0.5 0.5 0.45 54.1 Model_3 0.9 0.3 0.12 15.4 Data Normalisation # The data needs to be normalized before we can plot it. This is because we are plotting multiple metrics on the same scale, so our values also need to be on the same scale.\nlets normalize all of our data to be between 0 and 1.\nSSIM varies b/w -1 and 1, to normalize we just add 1 and divide by 2. MAE can be vary between 0 and infinity, lets assume max valye is 3 for MAE. Same case as MAE, max value assumed to be 5 PSNR for 8-bit grayscale images typically varies b/w 0-60 dB, so here lets - just normalize by dividing by 60. sample_df[\u0026#34;SSIM\u0026#34;] = (sample_df[\u0026#34;SSIM\u0026#34;] + 1) / 2 sample_df[\u0026#34;MAE\u0026#34;] = sample_df[\u0026#34;MAE\u0026#34;] / 3 sample_df[\u0026#34;MSE\u0026#34;] = sample_df[\u0026#34;MSE\u0026#34;] / 5 sample_df[\u0026#34;PSNR\u0026#34;] = sample_df[\u0026#34;PSNR\u0026#34;] / 60 This is what the data looks like after normalization.\nModel SSIM MAE MSE PSNR Model_1 0.45 0.70 0.94 0.54 Model_2 0.75 0.16 0.09 0.90 Model_3 0.95 0.10 0.02 0.25 Plotting the data # We will use good ol\u0026rsquo; matplotlib to create our plots. The following function is used to create the plots. The different parts of the codes are explained in the comments.\ndef radar_plot(df, attrs_to_plot, color_list=[\u0026#34;r\u0026#34;, \u0026#34;g\u0026#34;, \u0026#34;b\u0026#34;], title=\u0026#34;Radar plot\u0026#34;): # get the number of different metrics to plot n_metrics = len(attrs_to_plot) # we get the angles for each different metric angles = list(np.linspace(0, 2 * np.pi, n_metrics, endpoint=False)) # get label for each metric labels = list(attrs_to_plot) # we need the plot to be closed, so we close the plot # by adding the first metric at the end again angles += angles[:1] labels += labels[:1] # pass polar as true for ciruclar plot fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True)) # Set the labels for each of the angles ax.set_thetagrids(np.degrees(angles), labels=labels) # since we normalized all of our data to be between 0 and 1 # we set the limits of the plot to be between 0 and 1 ax.set_ylim(0, 1) for color, index_name in zip(color_list, df.index): # get the values for each row of the metrics # do note that we set the model name as the index vals = list(df[attrs_to_plot].loc[index_name]) # wrap around the values so that the plot is closed vals += vals[:1] # plot the polygon ax.plot(angles, vals, linewidth=1, color=color, label=index_name) # fill the area with color ax.fill(angles, vals, alpha=0.25, color=color) plt.legend(loc=\u0026#34;upper right\u0026#34;, bbox_to_anchor=(1.3, 1), fontsize=11) plt.title(title, fontsize=14, pad=10) Do note that the index of the sample_df has been changed to be the model name instead of numeric values. We then call the radar_plot as follows -\nradar_plot( sample_df, attrs_to_plot=[\u0026#34;SSIM\u0026#34;, \u0026#34;MAE\u0026#34;, \u0026#34;MSE\u0026#34;, \u0026#34;PSNR\u0026#34;], title=\u0026#34;Radar plot for model comparison\u0026#34;) One of the issues we see here is that there is a mix of negatively(lower is better) and positively(higher is better) oriented scores. This can create a bit of confusion when looking at the plot. Maybe if we invert the MSE and MAE using(1 - metric_value) then it might make more sense?\nPlotting after inverting using the following code -\nsample_df[\u0026#34;MAE_rev\u0026#34;] = 1 - sample_df[\u0026#34;MAE\u0026#34;] sample_df[\u0026#34;MSE_rev\u0026#34;] = 1 - sample_df[\u0026#34;MSE\u0026#34;] radar_plot( sample_df, attrs_to_plot=[\u0026#34;SSIM\u0026#34;, \u0026#34;MAE_rev\u0026#34;, \u0026#34;MSE_rev\u0026#34;, \u0026#34;PSNR\u0026#34;], title=\u0026#34;Radar plot for model comparison\u0026#34;, ) Conclusions # Hmm\u0026hellip;, honestly still confusing. I think this can lead to confusion in explaining what rev_mae and rev_mse is. I think the better approach would be to plot negatively and positively oriented metrics on different plots altogether.\nBut regardless, these plots can be used for so much more than just comparing model metrics and can be used for comparing input features or data samples together. Here are some pros and cons of these plots.\nPros-\nGives a nice summary of the different metrics/features for different models/data samples etc. Way better than looking at a table and trying to identify what each model is best at. Radar plots are visually very easy to understand and look nice aesthetically. Can be adjusted very easily for multiple metrics. Cons -\nMore models/samples can result in visual clutter. e.g - plotting the performance of 10 models like this would look horrible. All data needs to be normalized before plotting. Mixing of positive and negative oriented metrics can lead to confusion. It is very hard to compare to radar plots if the range and variables are different. Github Notebook ","date":"9 May 2023","permalink":"/blog/radar-plots/","section":"My Personal Blog","summary":"Motive # Does looking at a table comparing model peformance across multiple metrics feel bothersome to you? Lets see how radar plots can help us quickly visually compare model performance across multiple metrics.","title":"Using Radar Plots to Compare Model Performance"},{"content":"","date":"9 May 2023","permalink":"/tags/visualisation/","section":"Tags","summary":"","title":"visualisation"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":" This page is a work in progress Spacesense.ai Aug 2020 ‑ Oct 2020 Freelance ML and CV engineer Developed POC for an end to end object detection pipeline using satellite images for tracking repairs of damaged roofs. The inference pipeline solved issues for running ML models on very high resolution images and bounding box resolution. The work included dataset creation and annotation, satellite imagery pre and post processing, model benchmarking and a demo website. Dept. of Comp. Apps, MAHE Aug 2019 - Aug 2020 Research Project Implemented SOTA object detection architectures such as YOLOv3 and RetinaNet for localisation of humans in aerial and thermal images. As a key member of the research team, I tackled major challenges including boosting the speed of detection and accurately detecting small objects. My in-depth understanding of different object detection architectures allowed me to make significant contributions to our use case I helped in creating plots and creating the training pipeline for the SSD model for our paper \"Human Detection in Aerial Thermal Images Using Faster R-CNN and SSD Algorithms\" NIC, Govt. of India Dec 2019 - Jan 2020 Computer Vision and ML intern Developed semantic segmentation models for multi-spectral very high resolution satellite imagery for buildings, agricultural land and trees using Unet based model architectures. Migrated and structured old tree based algorithms into a python library Helped in improving the MIoU by 50% from the previous approach Worked on creating and testing a basic approach for Devanagri alphabet recognition. Rimtex Engineering Sept 2019 - Oct 2019 Robotics Intern Developed the complete ROS stack for a warehouse UGV prototype including simulation, perception, localisation and path planning modules. TATA SONS GTIO May 2019 - June 2019 Computer Vision and Robotics Intern Created the technical roadmap of the project based on the task \"automated surveillance of buildings by unskilled workers using drones.\" Developed and deployed an autonomous one-shot object matching, detection and tracking system using drones as the base platform and the Jetson Nano as the processing board. Created an API for using the DJI Tello SDK with the PixHawk ROS package. Presented live tech demos of the product for the core management team and potential clients Worked in an office environment for the first time in my life, working under seasoned industry innovators. R.U.G.V.E.D Jan 2018 - Apr 2019 AI Subsystem Head Led the AI team consisting of 4 members working on developing an autonomous UGV for automated reconaissance and disaster management. Won the TATA Makerthon 2018 held at IIT Bombay by creating a zero-shot 360 degree object matcher for drones, beating 142 teams across India. Developed the full software stack(mapping, perception, localistion) of an autonmous UGV based on ROS for the Intelligent Ground Vehicle Competition (IGVC). Integrated software and hardware stacks and tested them in real-time on the UGV. Participated and placed 8th in the IGVC competition (design report). Conducted tests and task phases to recruit new members to the team. Had a shitload of fun goofing around with friends and taking advantage of the extended hostel permits. ","date":"1 January 0001","permalink":"/about/","section":"Sambhav Singh Rohatgi","summary":"This page is a work in progress Spacesense.ai Aug 2020 ‑ Oct 2020 Freelance ML and CV engineer Developed POC for an end to end object detection pipeline using satellite images for tracking repairs of damaged roofs.","title":"My Professional Timeline"},{"content":"CV and ML researcher and engineer\n","date":"1 January 0001","permalink":"/authors/sambhav/","section":"Authors","summary":"CV and ML researcher and engineer","title":"Sambhav Singh Rohatgi"},{"content":"","date":"1 January 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"}]