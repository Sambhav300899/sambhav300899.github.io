[{"content":"","date":"6 June 2023","permalink":"/tags/clustering/","section":"Tags","summary":"","title":"clustering"},{"content":" Motive #  The motive of the following post is to take a look at how to cluster geospatial data using the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm.\nAs a use case for this post we will look at how we can find the most representative data points for each continent to test geographical variation in a dataset for a theoretical ML model.\nWe will use the SEN12TP dataset for demostation here.\n What is DBSCAN and why is it useful for geospatial clustering? #  The DBSCAN algorithm is a popular density-based clustering algorithm used for identifying clusters and outliers in spatial data. It is particularly well-suited for geospatial data because it can handle data with irregularly shaped clusters and varying densities effectively, something that traditional algorithms such as k-means are not able to do.\nDBSCAN defines clusters as dense regions of data points separated by regions of lower density. The algorithm works by exploring the density connectivity of the data points. To learn more about how it works visit the following https://www.analyticsvidhya.com/blog/2020/09/how-dbscan-clustering-works/.\nThe advantages of DBSCAN for geospatial data include-\n Robustness to noise: DBSCAN can effectively handle outliers and noise points as they are considered separate from clusters. It does not assign them to any specific cluster and does not force them to belong to a cluster. Ability to detect clusters of arbitrary shape: DBSCAN can identify clusters with irregular shapes, making it suitable for geospatial data where clusters may have complex spatial distributions. Handling of varying cluster densities: DBSCAN can handle clusters of different densities. It can adapt to regions with high-density clusters as well as regions with low-density clusters.   Our problem statement #    Let\u0026rsquo;s say we have a hypothetical model for sentinel-1 to sentinel-2 image to image transalation that we want to test on this dataset.\n  In particlar we want to test the model for geographical variance in performance per continent.\n  The dataset contains too many points per continent and we need some way to reduce these to get the most representative points for each continent.\n  We use the DBSCAN algorithm to find N most representative points per continent.\n   Downloading the dataset #  The SEN12TP dataset contains paired set of imagery from the sentinel-1 and sentinel-2 satellite, but we won\u0026rsquo;t be using the imagery here, wplt.figure(figsize=(20, 20)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.stock_img()\nax.plot(gdf[\u0026ldquo;lon\u0026rdquo;], gdf[\u0026ldquo;lat\u0026rdquo;], \u0026ldquo;o\u0026rdquo;, color=\u0026ldquo;r\u0026rdquo;) plt.title(\u0026ldquo;SEN12TP dataset locations\u0026rdquo;)to get geolocations from all over the world.\nDownload the metadata for the dataset using the following command -\nwget https://raw.githubusercontent.com/Sambhav300899/blog_notebooks/main/clustering/dbscan/sen12tp-metadata.json  Loading the dataset #  We can use geopandas to directly load the dataset.\nimport geopandas as gpd gdf = gpd.read_file(\u0026#34;sen12tp-metadata.json\u0026#34;) The dataset is in a different CRS than WGS84, converting it to WGS84 format will allow us to look at it as lattitude and longitude values. The epsg code for WGS84 format is 4326. We can use geopandas to convert our data to the WGS84 projection.\nEach row of the geopandas dataframe represents one image location. We extract the latitude and longitude of the centroid of each image location.\n# convert to crs 4326 to get lat lon gdf = gdf.to_crs(epsg=4326) gdf = gdf[[\u0026#34;geometry\u0026#34;]] # get lat lon of centroids gdf[\u0026#34;centroid\u0026#34;] = gdf[\u0026#34;geometry\u0026#34;].centroid gdf[\u0026#34;lat\u0026#34;] = gdf[\u0026#34;centroid\u0026#34;].y gdf[\u0026#34;lon\u0026#34;] = gdf[\u0026#34;centroid\u0026#34;].x Lets see what all data we have in the dataframe now -\ngdf.head()     geometry centroid lat lon     0 POLYGON ((-70.41461164636456 -34.46821038082358,\u0026hellip; POINT (-70.30 -34.55) -34.5596 -70.3071   1 POLYGON ((72.28825977983655 34.050558629310395,\u0026hellip; POINT (72.399 33.96) 33.9627 72.3993   2 POLYGON ((-8.424361551813695 41.53310978422038,\u0026hellip; POINT (-8.305 41.44) 41.4423 -8.3054   3 POLYGON ((6.724249816765044 46.5562534318746,\u0026hellip; POINT (6.85 46.46) 46.4688 6.85822   4 POLYGON ((-0.554267167211116 42.60345308513507,\u0026hellip; POINT (-0.43 42.51) 42.5108 -0.43611     Plotting all of the locations #  We will use the cartopy library to plot a map of the earth and the locations of our datapoints on it.\nimport cartopy.crs as ccrs plt.figure(figsize=(20, 20)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.stock_img() ax.plot(gdf[\u0026#34;lon\u0026#34;], gdf[\u0026#34;lat\u0026#34;], \u0026#34;o\u0026#34;, color=\u0026#34;r\u0026#34;) plt.title(\u0026#34;SEN12TP dataset locations\u0026#34;)  Reverese geocoding #  We want to get N points per continent, it is important to know what continent each location belongs to before we do this. To do this we will use the reverse_geocoder library.\nimport reverse_geocoder as rg continent_dict = { \u0026#34;NA\u0026#34;: \u0026#34;North America\u0026#34;, \u0026#34;SA\u0026#34;: \u0026#34;South America\u0026#34;, \u0026#34;AS\u0026#34;: \u0026#34;Asia\u0026#34;, \u0026#34;AF\u0026#34;: \u0026#34;Africa\u0026#34;, \u0026#34;OC\u0026#34;: \u0026#34;Oceania\u0026#34;, \u0026#34;EU\u0026#34;: \u0026#34;Europe\u0026#34;, \u0026#34;AQ\u0026#34;: \u0026#34;Antarctica\u0026#34;, } gdf[\u0026#34;continent_code\u0026#34;] = rg.search(list(map(tuple, gdf[[\u0026#34;lat\u0026#34;, \u0026#34;lon\u0026#34;]].values))) gdf[\u0026#34;continent_code\u0026#34;] = gdf[\u0026#34;continent_code\u0026#34;].apply( lambda x: pc.country_alpha2_to_continent_code(x[\u0026#34;cc\u0026#34;]) ) gdf[\u0026#34;continent_name\u0026#34;] = gdf[\u0026#34;continent_code\u0026#34;].apply(lambda x: continent_dict[x]) Let\u0026rsquo;s plot the points again to confirm that we geocoded correctly.\nplt.figure(figsize=(20, 20)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.stock_img() for continent_name in gdf[\u0026#34;continent_name\u0026#34;].unique(): ax.plot( gdf[gdf[\u0026#34;continent_name\u0026#34;] == continent_name][\u0026#34;lon\u0026#34;], gdf[gdf[\u0026#34;continent_name\u0026#34;] == continent_name][\u0026#34;lat\u0026#34;], \u0026#34;o\u0026#34;, label=continent_name, ) plt.legend() plt.title(\u0026#34;SEN12TP dataset locations\u0026#34;)  Running DBSCAN per continent #  We will now run the DBSCAN algorithm per continent to get 50 representative points for the same.\nWe first define functions to run the DBSCAN algorithm -\ndef get_centermost_point(cluster): # This function is useful for getting the centerpoint of a cluster # We first extract the centroid of the cluster centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y) # we then calculate the distrances between all points and the centerpoint of the cluster and take the one with the least distance centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m) return tuple(centermost_point) def get_n_filtered_pts(gdf_dbscan, max_pts=10, max_dist_km=200): # This function runs dbscan on a geodataframe and retruns `max_pts` representative locations for it.  # the max_dist_km is the maximum distance that a point can be from the center of the cluster # we will also convert all of our data to radians # we first extract the latitudes and longitudes in the form of a numpy array coords = gdf_dbscan[[\u0026#34;lat\u0026#34;, \u0026#34;lon\u0026#34;]].values # we then calculate the `epsilon` paramter which is converting our `max_dist_km` to radians # look at this to understadnd - https://stackoverflow.com/a/49212829 kms_per_radian = 6371.0088 epsilon = max_dist_km / kms_per_radian # Run DBSCAN db = DBSCAN( eps=epsilon, min_samples=1, algorithm=\u0026#34;ball_tree\u0026#34;, metric=\u0026#34;haversine\u0026#34; ).fit(np.radians(coords)) # get labels for each point in our dataset cluster_labels = db.labels_ # get the number of clusters by looking at the number of unique clusters num_clusters = len(set(cluster_labels)) # get the different points in each cluster clusters = pd.Series([coords[cluster_labels == n] for n in range(num_clusters)]) # get the centermost point in each cluster filtered_pts = list(clusters.map(lambda x: get_centermost_point(x))) # if we have points more than the required amount, randomly sample if len(filtered_pts) \u0026gt; max_pts: filtered_pts = random.sample(filtered_pts, max_pts) return filtered_pts We then use the following code to run DBSCAN per continent and create a geodataframe of the filtered data points.\nnum_pts_per_continent = 50 all_filtered_pts = [] # run per continent for continent_name in gdf[\u0026#34;continent_name\u0026#34;].unique(): continent_gdf = gdf[gdf[\u0026#34;continent_name\u0026#34;] == continent_name] all_filtered_pts.extend(get_n_filtered_pts(continent_gdf, num_pts_per_continent)) # get all lat and lons for all continents lats, lons = zip(*all_filtered_pts) rep_points = pd.DataFrame({\u0026#34;lon\u0026#34;: lons, \u0026#34;lat\u0026#34;: lats}) # extract all other information about these points from the original geodataframe filtered_gdf = rep_points.apply( lambda row: gdf[(gdf[\u0026#34;lat\u0026#34;] == row[\u0026#34;lat\u0026#34;]) \u0026amp; (gdf[\u0026#34;lon\u0026#34;] == row[\u0026#34;lon\u0026#34;])].iloc[0], axis=1, ) Let\u0026rsquo;s visualise this now to see the final datapoints we get.\nplt.figure(figsize=(20, 20)) ax = plt.axes(projection=ccrs.PlateCarree()) ax.stock_img() for continent_name in filtered_gdf[\u0026#34;continent_name\u0026#34;].unique(): ax.plot( filtered_gdf[filtered_gdf[\u0026#34;continent_name\u0026#34;] == continent_name][\u0026#34;lon\u0026#34;], filtered_gdf[filtered_gdf[\u0026#34;continent_name\u0026#34;] == continent_name][\u0026#34;lat\u0026#34;], \u0026#34;o\u0026#34;, label=continent_name, ) plt.legend() plt.title(\u0026#34;SEN12TP dataset locations\u0026#34;)  Conclusion #  We see that we get a pretty good representation of the geograhical locations of our dataset in these points. We can now furthur use this to test our models geograhical performance.\n  Github Notebook  ","date":"6 June 2023","permalink":"/blog/dbscan-clustering/","section":"My Personal Blog","summary":"Motive #  The motive of the following post is to take a look at how to cluster geospatial data using the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm.","title":"DBSCAN clustering and reverse geocoding for geospatial data"},{"content":"","date":"6 June 2023","permalink":"/tags/geospatial/","section":"Tags","summary":"","title":"geospatial"},{"content":"","date":"6 June 2023","permalink":"/blog/","section":"My Personal Blog","summary":"","title":"My Personal Blog"},{"content":"","date":"6 June 2023","permalink":"/","section":"Sambhav Singh Rohatgi","summary":"","title":"Sambhav Singh Rohatgi"},{"content":"","date":"6 June 2023","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"6 June 2023","permalink":"/tags/visualization/","section":"Tags","summary":"","title":"visualization"},{"content":"","date":"28 May 2023","permalink":"/tags/paper-notes/","section":"Tags","summary":"","title":"paper notes"},{"content":"This section contains notes from papers I have read, these can be very unrefined and unedited.\n","date":"28 May 2023","permalink":"/papernotes/","section":"Paper Reading Notes","summary":"This section contains notes from papers I have read, these can be very unrefined and unedited.","title":"Paper Reading Notes"},{"content":"These are notes for my reading of the segment anything paper. Check out the orignal project page by Meta AI at the link below\nSegment Anything by Meta AI     High Level #    Main idea behind the model is to build a foundational model for Computer Vision similar to the state of the NLP community + allow it to be promptable.   This model can then be used for downstream segmentation problems using prompt engineering and for some cases zero shot finetuning.\n  They mention 3 main parts of the project as -\n  What task will enable zero-shot generalization?\n  Chose promptable segmentation, as it is a suitable pretraining task for a lot of CV problems.\n  Goal of the task is to return a valid segmentation mask given any segmentation prompt.\n    What is the corresponding model architecture?\n  Real-time flexible promptable model\n  Flexible here means it must be able give a valid segmentation mask for any prompt.   Since the model is supposed to be interactive, it needs real time outputs.   Model -\n  a powerful image encoder computes an image embedding\n  a prompt encoder embeds prompts\n  Image an prompt embedding sources are combined in a lightweight mask decoder that predicts segmentation masks\n    Types of supported prompts -\n  Box\n  Mask\n  Point\n  Free form text\n    What data can power this task and model?\n  To build the dataset, they use the model for labelling. They call this the \u0026ldquo;data engine\u0026rdquo;\n  To achieve strong generalisations to new data distributions, it was necessary to train SAM on large and diverse set of masks.   No existing dataset satisfied these requirements.\n  They use \u0026lsquo;model in the loop\u0026rsquo; annotation.\n  3 stages to the \u0026ldquo;data engine\u0026rdquo; -   Assisted-manual\n Model assists annotators in annotation.     Semi-auto.\n  Model can generate annotations for a subset of objects by prompting with likely object locations and masks. (is this because of good performance after stage 1?)\n  Annotators annotate the rest of the objects.\n    Fully auto\n  Model is prompted with a regular grid of foreground points. (what does this mean?)\n  This results in generation of ~100 high quality masks per image             Final dataset - The final dataset, SA-1B, includes more than 1B masks from 11M licensed and privacy-preserving images. The masks were manually verified to have good annotations.   Focused on AI ethics to look at potential geographical and racial biases the model might have\n  Experiments\n  Tested using 23 segmentation datasets to find out that it generated high-quality masks from a single foreground point, these annotations were only slightly below the ground truth.   Got good results on zeroshot downstream tasks such as edge detection, object proposal generation, instance segmentation and preliminary exploration of text-to-mask prediction\n  These results suggest that SAM can be used out-of-the-box with prompt engineering to solve a variety of tasks involving object and image distributions beyond the model\u0026rsquo;s training data.\n     Task #   The promptable segmentation task is to return a valid segmentation mask given any prompt. “Valid” here means that a segmentation mask should be returned even if the prompt is ambiguous. Look at the figure below to understand what ambiguous means here.    This task was chosen since it acts as a natural pre-training algorithm and a general method for zero-shot transfer to downstream tasks via prompting.\n  One suggested method of using for downstream instance segmentation\n For example, if one has a bounding box detector for cats, cat instance segmentation can be solved by providing the detector’s box output as a prompt to our model.    An important distinction pointed out by them is that the model is not trained in a multitask scenario but can perform different tasks at inference tasks by pairing it with different components.\n   Model #    Three main components -   image encoder\n  They use masked autoencoder (MAE) pre-trained ViT adapted to process high res outputs.   What is a masked autoencoder?\n    a flexible prompt encoder\n  Prompts are broken down into 2 categories-   Sparse - points, boxes and text   Dense - mask\n  Points and boxes are encoded using positional embeddings   Text is converted to embeddings using CLIP encoder.\n  Dense prompts are encoded using convolutional operations.\n    fast mask decoder\n  The mask decoder efficiently maps the image embedding, prompt embeddings, and an output token to a mask.\n  The modified decoder block uses prompt self-attention and cross-attention in two directions (prompt-to-image embedding and vice-versa) to update all embeddings.\n  After running two blocks, we upsample the image embedding and an MLP maps the output token to a dynamic linear classifier, which then computes the mask foreground probability at each image location\n      The model was modified to allow for multiple masks being generated for a single prompt. This is to tackle ambiguity in the prompts. They set the limit of masks generated to 3, as they found it to be sufficient for most usecases.   The model is designed to be efficient. The image encoder is the computationally heavy part of the model, afterwards the prompt encoder and mask decoder can run efficiently even in a browser.\n  This design allows testing the model with different prompts very easily, since the image embeddings don\u0026rsquo;t need to be computed repeatedly.   Mask prediction - dice + focal loss was used for training.\n   Data Engine #    The data engine has three stages -   model-assisted manual annotation stage\n  Normal image annotation assisted by the model in an interactive browser UI environment\n  No named labels for the objects were collected.\n  The model was trained on common segmentation datasets before this and then on the slice of the data that had been annotated in stage 1.\n  4.3M masks were collected from 120k images in this stage.\n  Model was trained 5-6 times during this phase\n    semi-automatic stage with a mix of automatically predicted masks and model-assisted annotation   The aim here was to increase the diversity of masks in order to improve the models ability to segment anything.\n  The model was allowed to make predictions first and then the less common objects were labelled by the annotators.\n  Model was trained 5-6 times during this phase.\n    fully automatic stage in which our model generates masks without annotator input\n  Enough data had been collected by this point to allow for the model to train automatically.\n  The model was prompted with 32 x 32 grid of points and tasked to predict the masks for these.\n  The concept of stable mask is introduced, a mask is stable if thresholding the probability map at 0.5 - delta and 0.5 + delta results in similar masks.   delta = 0.3\n  mask_over = mask \u0026gt; 0.5 + delta\n  mask_under = mask \u0026gt; 0.5 - delta\n  similarity(mask_over, mask_under)\n    After selecting confident masks NMS was applied to filter duplicates.   Fully automatic mask generation was applied to 11M images to generated 1.1B masks.\n      The SA-1B dataset has been released online, it consists of only generated masks.   A comparison of the generated masks was done with manually annotated masks and showed that the quality of the generated masks is good.   The authors performed a bias and fairness analysis of the dataset as well.    Zero-shot Experiments #    They analysed the performance of the model on different zero-shot experiments.   The tasks are as follows -   Edge detection\n  Generated segmentation masks and removed duplicates using NMS. These were generated by prompting the model with 16x16 grids of foreground points.   Edge maps are then computed using sobel filtering of un-thresholded mask probability maps and postprocessing.\n  It produced reasonable edge maps.     Object proposal generation\n To generate object proposals, they ran a slightly modified version of the automatic mask generation pipeline and output the masks as proposals.    Instance segmentation\n  Segment objects from free-form text\n    ","date":"28 May 2023","permalink":"/papernotes/segment-anything/","section":"Paper Reading Notes","summary":"These are notes for my reading of the segment anything paper. Check out the orignal project page by Meta AI at the link below\nSegment Anything by Meta AI     High Level #    Main idea behind the model is to build a foundational model for Computer Vision similar to the state of the NLP community + allow it to be promptable.","title":"Segment Anything"},{"content":"","date":"28 May 2023","permalink":"/tags/segmentation/","section":"Tags","summary":"","title":"segmentation"},{"content":" Motive #  Does looking at a table comparing model peformance across multiple metrics feel bothersome to you? Lets see how radar plots can help us quickly visually compare model performance across multiple metrics.\nThis is just a single use case of using radar plots though, these can be used for so much more such as - comparing multiple products across multiple metrics, comparing multiple people across multiple skills, etc.\nThe first time I appreciating radar plots was in Fifa 18, where you could compare players across pace, skill, strength, etc.very easily. Made quick substitutions very simple.\nVisually they look very cool and are certain to add visual flair to your next presentation.\n What are radar plots? #  Defining them a bit more formally, a radar plot is a type of chart used to display multiple variables on a two-dimensional graph. Each variable is plotted on a separate axis that radiates from the center of the graph, and data points are connected to create a polygon shape.\nRadar plots are commonly used to compare the performance of different models or entities across multiple metrics. They are helpful for visualizing how different variables affect overall performance and are widely used in fields such as data analysis, engineering, and sports.\n Sample data #  Here we will just be using a randomly generated table of data. Lets say we have a hypothetical image to image translation problem. We will compare 3 models across 4 metrics SSIM, PSNR, MAE and MSE. We load this in the form of a pandas dataframe named sample_df.\n   NOTE! Keep in mind the data makes no sense in terms of metrics and is only meant for visualisation purposes!     Model SSIM MAE MSE PSNR     Model_1 -0.1 2.1 4.70 32.6   Model_2 0.5 0.5 0.45 54.1   Model_3 0.9 0.3 0.12 15.4     Data Normalisation #  The data needs to be normalized before we can plot it. This is because we are plotting multiple metrics on the same scale, so our values also need to be on the same scale.\nlets normalize all of our data to be between 0 and 1.\n SSIM varies b/w -1 and 1, to normalize we just add 1 and divide by 2. MAE can be vary between 0 and infinity, lets assume max valye is 3 for MAE. Same case as MAE, max value assumed to be 5 PSNR for 8-bit grayscale images typically varies b/w 0-60 dB, so here lets - just normalize by dividing by 60.  sample_df[\u0026quot;SSIM\u0026quot;] = (sample_df[\u0026quot;SSIM\u0026quot;] + 1) / 2 sample_df[\u0026quot;MAE\u0026quot;] = sample_df[\u0026quot;MAE\u0026quot;] / 3 sample_df[\u0026quot;MSE\u0026quot;] = sample_df[\u0026quot;MSE\u0026quot;] / 5 sample_df[\u0026quot;PSNR\u0026quot;] = sample_df[\u0026quot;PSNR\u0026quot;] / 60 This is what the data looks like after normalization.\n   Model SSIM MAE MSE PSNR     Model_1 0.45 0.70 0.94 0.54   Model_2 0.75 0.16 0.09 0.90   Model_3 0.95 0.10 0.02 0.25     Plotting the data #  We will use good ol' matplotlib to create our plots. The following function is used to create the plots. The different parts of the codes are explained in the comments.\ndef radar_plot(df, attrs_to_plot, color_list=[\u0026#34;r\u0026#34;, \u0026#34;g\u0026#34;, \u0026#34;b\u0026#34;], title=\u0026#34;Radar plot\u0026#34;): # get the number of different metrics to plot n_metrics = len(attrs_to_plot) # we get the angles for each different metric angles = list(np.linspace(0, 2 * np.pi, n_metrics, endpoint=False)) # get label for each metric labels = list(attrs_to_plot) # we need the plot to be closed, so we close the plot # by adding the first metric at the end again angles += angles[:1] labels += labels[:1] # pass polar as true for ciruclar plot fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True)) # Set the labels for each of the angles ax.set_thetagrids(np.degrees(angles), labels=labels) # since we normalized all of our data to be between 0 and 1 # we set the limits of the plot to be between 0 and 1 ax.set_ylim(0, 1) for color, index_name in zip(color_list, df.index): # get the values for each row of the metrics # do note that we set the model name as the index vals = list(df[attrs_to_plot].loc[index_name]) # wrap around the values so that the plot is closed vals += vals[:1] # plot the polygon ax.plot(angles, vals, linewidth=1, color=color, label=index_name) # fill the area with color ax.fill(angles, vals, alpha=0.25, color=color) plt.legend(loc=\u0026#34;upper right\u0026#34;, bbox_to_anchor=(1.3, 1), fontsize=11) plt.title(title, fontsize=14, pad=10) Do note that the index of the sample_df has been changed to be the model name instead of numeric values. We then call the radar_plot as follows -\nradar_plot( sample_df, attrs_to_plot=[\u0026#34;SSIM\u0026#34;, \u0026#34;MAE\u0026#34;, \u0026#34;MSE\u0026#34;, \u0026#34;PSNR\u0026#34;], title=\u0026#34;Radar plot for model comparison\u0026#34;) One of the issues we see here is that there is a mix of negatively(lower is better) and positively(higher is better) oriented scores. This can create a bit of confusion when looking at the plot. Maybe if we invert the MSE and MAE using(1 - metric_value) then it might make more sense?\nPlotting after inverting using the following code -\nsample_df[\u0026#34;MAE_rev\u0026#34;] = 1 - sample_df[\u0026#34;MAE\u0026#34;] sample_df[\u0026#34;MSE_rev\u0026#34;] = 1 - sample_df[\u0026#34;MSE\u0026#34;] radar_plot( sample_df, attrs_to_plot=[\u0026#34;SSIM\u0026#34;, \u0026#34;MAE_rev\u0026#34;, \u0026#34;MSE_rev\u0026#34;, \u0026#34;PSNR\u0026#34;], title=\u0026#34;Radar plot for model comparison\u0026#34;, )  Conclusions #  Hmm\u0026hellip;, honestly still confusing. I think this can lead to confusion in explaining what rev_mae and rev_mse is. I think the better approach would be to plot negatively and positively oriented metrics on different plots altogether.\nBut regardless, these plots can be used for so much more than just comparing model metrics and can be used for comparing input features or data samples together. Here are some pros and cons of these plots.\nPros-\n Gives a nice summary of the different metrics/features for different models/data samples etc. Way better than looking at a table and trying to identify what each model is best at. Radar plots are visually very easy to understand and look nice aesthetically. Can be adjusted very easily for multiple metrics.  Cons -\n More models/samples can result in visual clutter. e.g - plotting the performance of 10 models like this would look horrible. All data needs to be normalized before plotting. Mixing of positive and negative oriented metrics can lead to confusion. It is very hard to compare to radar plots if the range and variables are different.    Github Notebook  ","date":"9 May 2023","permalink":"/blog/radar-plots/","section":"My Personal Blog","summary":"Motive #  Does looking at a table comparing model peformance across multiple metrics feel bothersome to you? Lets see how radar plots can help us quickly visually compare model performance across multiple metrics.","title":"Using Radar Plots to Compare Model Performance"},{"content":"","date":"9 May 2023","permalink":"/tags/visualisation/","section":"Tags","summary":"","title":"visualisation"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"    This page is a work in progress     SpaceSense  Aug 2021 - now     ML Engineer and Researcher   Developed a pipeline to automate training of time series crop type segmentation models from vector label data. Developed a pipeline to automate the training of landcover segmentation models from vector label data and packaged this as a workflow for customers Published CCAI@NeurIPS 2022 tutorial for LULC dataset creation and model training. Published technical blogs and took part as a presenter in company webinars. Developed POCs on S2 Super‑resolution and S2 Data Augmentation Data Preparation for AI Feature development and refinement for Data Preparation for AI, a toolbox specialized to remote sensing data. Deployment and monitoring of Beyond Cloud for testing by clients.     --       SpaceSense  Mar 2021 ‑ Jul 2021     ML Intern (R\u0026amp;D TEAM)   Beyond Cloud‑ Developed GAN based model to predict S2 vegetative indices from S1 SAR. new models outperformed the previous model with a 30‑40% improvement in the average loss for the test set. Developed a modular and distributed model training pipeline including experimental tracking and artifact storage on GCP. Developed a modular data acquisition pipeline which was used to create a dataset of 120 GB for our image to image translation task. Extensive work on data visualisation and model benchmarking w.r.t both computer vision and remote sensing perspectives.        SpaceSense  Aug 2020 ‑ Oct 2020     Freelance ML and CV engineer   Developed POC for an end to end object detection pipeline using satellite images for tracking repairs of damaged roofs. The inference pipeline solved issues for running ML models on very high resolution images and bounding box resolution. The work included dataset creation and annotation, satellite imagery pre and post processing, model benchmarking and a demo website.        Dept. of Comp. Apps, MAHE  Aug 2019 - Aug 2020     Research Project   Implemented SOTA object detection architectures such as YOLOv3 and RetinaNet for localisation of humans in aerial and thermal images. As a key member of the research team, I tackled major challenges including boosting the speed of detection and accurately detecting small objects. My in-depth understanding of different object detection architectures allowed me to make significant contributions to our use case I helped in creating plots and creating the training pipeline for the SSD model for our paper \"Human Detection in Aerial Thermal Images Using Faster R-CNN and SSD Algorithms\"    $(document).ready(function () { $('#gallery-1686074355763463525').packery({ percentPosition: true, gutter: 5, resize: true }); })        NIC, Govt. of India  Dec 2019 - Jan 2020     Computer Vision and ML intern   Developed semantic segmentation models for multi-spectral very high resolution satellite imagery for buildings, agricultural land and trees using Unet based model architectures. Migrated and structured old tree based algorithms into a python library Helped in improving the MIoU by 50% from the previous approach Worked on creating and testing a basic approach for Devanagri alphabet recognition.        Rimtex Engineering  Sept 2019 - Oct 2019     Robotics Intern   Developed the complete ROS stack for a warehouse UGV prototype including simulation, perception, localisation and path planning modules.        TATA SONS GTIO  May 2019 - June 2019     Computer Vision and Robotics Intern   Created the technical roadmap of the project based on the task \"automated surveillance of buildings by unskilled workers using drones.\" Developed and deployed an autonomous one-shot object matching, detection and tracking system using drones as the base platform and the Jetson Nano as the processing board. Created an API for using the DJI Tello SDK with the PixHawk ROS package. Presented live tech demos of the product for the core management team and potential clients Worked in an office environment for the first time in my life, working under seasoned industry innovators.            R.U.G.V.E.D  Jan 2018 - Apr 2019     AI Subsystem Head    Led the AI team consisting of 4 members working on developing an autonomous UGV for automated reconaissance and disaster management.  Won the TATA Makerthon 2018  held at IIT Bombay by creating a zero-shot 360 degree object matcher for drones, beating 142 teams across India.  Developed the full software stack(mapping, perception, localistion) of an autonmous UGV based on ROS for the Intelligent Ground Vehicle Competition (IGVC).  Integrated software and hardware stacks and tested them in real-time on the UGV.   Participated and placed 8th in the IGVC competition (design report).  Conducted tests and task phases to recruit new members to the team.  Had a shitload of fun goofing around with friends and taking advantage of the extended hostel permits.    $(document).ready(function () { $('#gallery-1686074355764461062').packery({ percentPosition: true, gutter: 5, resize: true }); })          ","date":"1 January 0001","permalink":"/about/","section":"Sambhav Singh Rohatgi","summary":"This page is a work in progress     SpaceSense  Aug 2021 - now     ML Engineer and Researcher   Developed a pipeline to automate training of time series crop type segmentation models from vector label data.","title":"My Professional Timeline"},{"content":"CV and ML researcher and engineer\n","date":"1 January 0001","permalink":"/authors/sambhav/","section":"Authors","summary":"CV and ML researcher and engineer","title":"Sambhav Singh Rohatgi"},{"content":"","date":"1 January 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"}]